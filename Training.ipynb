{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryk42/SemEval-Food-Hazard-Detection-Challenge/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0eZi3gaLRgEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_tnUGj2aOD40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uk4Hr3o6UHO",
        "outputId": "dfef0ccd-0ed5-4b10-feef-6db92058aebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting on google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mRqpDhY5OF0r"
      },
      "outputs": [],
      "source": [
        "# Configuration of the model\n",
        "config = {\n",
        "    'max_len': 256,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 0.00005,\n",
        "    'epochs': 100,\n",
        "    'model_name': \"dmis-lab/biobert-base-cased-v1.1\"  # BioBERT model name\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Z5Prdp5Xrb",
        "outputId": "8e6c322c-355b-460c-f2e5-85fadbd27fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YaxolIHS5SBH",
        "outputId": "a256d785-e130-41bd-8722-00d5c1e7446b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  year  month  day country                             title  \\\n",
            "0           0  1994      1    7      us  Recall Notification: FSIS-024-94   \n",
            "1           1  1994      3   10      us  Recall Notification: FSIS-033-94   \n",
            "2           2  1994      3   28      us  Recall Notification: FSIS-014-94   \n",
            "3           3  1994      4    3      us  Recall Notification: FSIS-009-94   \n",
            "4           4  1994      7    1      us  Recall Notification: FSIS-001-94   \n",
            "\n",
            "                                                text hazard-category  \\\n",
            "0  Case Number: 024-94   \\n            Date Opene...      biological   \n",
            "1  Case Number: 033-94   \\n            Date Opene...      biological   \n",
            "2  Case Number: 014-94   \\n            Date Opene...      biological   \n",
            "3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n",
            "4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n",
            "\n",
            "               product-category                  hazard  \\\n",
            "0  meat, egg and dairy products  listeria monocytogenes   \n",
            "1  meat, egg and dairy products            listeria spp   \n",
            "2  meat, egg and dairy products  listeria monocytogenes   \n",
            "3  meat, egg and dairy products        plastic fragment   \n",
            "4  meat, egg and dairy products        plastic fragment   \n",
            "\n",
            "                       product  \n",
            "0               smoked sausage  \n",
            "1                      sausage  \n",
            "2                   ham slices  \n",
            "3  thermal processed pork meat  \n",
            "4               chicken breast  \n"
          ]
        }
      ],
      "source": [
        "# URL of the raw file\n",
        "url = \"https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_train.csv\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Z6l4Gt-Y6EIH"
      },
      "outputs": [],
      "source": [
        "# Drop the first column\n",
        "df = df.drop(df.columns[0], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsoRrUZg5Xw6",
        "outputId": "cd9a27a9-9c17-4088-c3cc-c9df86c06af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5082 entries, 0 to 5081\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   year              5082 non-null   int64 \n",
            " 1   month             5082 non-null   int64 \n",
            " 2   day               5082 non-null   int64 \n",
            " 3   country           5082 non-null   object\n",
            " 4   title             5082 non-null   object\n",
            " 5   text              5082 non-null   object\n",
            " 6   hazard-category   5082 non-null   object\n",
            " 7   product-category  5082 non-null   object\n",
            " 8   hazard            5082 non-null   object\n",
            " 9   product           5082 non-null   object\n",
            "dtypes: int64(3), object(7)\n",
            "memory usage: 397.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Check the structure of the DataFrame\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WbqvZzJgOMmA"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset for Text Data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KlHfuLKlOQmI"
      },
      "outputs": [],
      "source": [
        "# Function to clean text (title or text) and remove stopwords\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "J5f2YpcNOVs5"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer for Microsoft PubMedBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df['text'] = df['text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xe4fnbojOY-v"
      },
      "outputs": [],
      "source": [
        "# Define relevant features and targets\n",
        "features = ['year', 'month', 'day', 'country']\n",
        "subtask1 = ['hazard-category','product-category']\n",
        "subtask2 = ['hazard','product']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b7B8EhUdOer2"
      },
      "outputs": [],
      "source": [
        "# Encode target labels to numeric values\n",
        "label_encoders = {}\n",
        "for target in subtask1 + subtask2:\n",
        "    le = LabelEncoder()\n",
        "    df[target] = le.fit_transform(df[target])\n",
        "    label_encoders[target] = le\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8a9dTrQ9Oh7Q"
      },
      "outputs": [],
      "source": [
        "# Prepare data for both title and text\n",
        "def prepare_data(text_column):\n",
        "    X = df[features + [text_column]]\n",
        "    y_subtask1 = df[subtask1]\n",
        "    y_subtask2 = df[subtask2]\n",
        "\n",
        "    data_splits = {}\n",
        "    for target in subtask1 + subtask2:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, df[target], test_size=0.1, random_state=42\n",
        "        )\n",
        "\n",
        "        # Reset indices to ensure matching\n",
        "        X_train = X_train.reset_index(drop=True)\n",
        "        y_train = y_train.reset_index(drop=True)\n",
        "        X_test = X_test.reset_index(drop=True)\n",
        "        y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return data_splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ffx_UagwOlb2"
      },
      "outputs": [],
      "source": [
        "# Prepare data for title and text\n",
        "title_splits = prepare_data('title')\n",
        "text_splits = prepare_data('text')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RQYXs45FOoPz"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_nn(data_splits, targets, model_type='title', early_stopping_patience=6, lr_reduce_factor=0.05):\n",
        "    f1_scores = []  # List to store F1 scores for each task\n",
        "\n",
        "    for target in targets:\n",
        "        print(f\"\\nStarting training for task: {target}\")  # Print task message\n",
        "\n",
        "        X_train, X_test, y_train, y_test = data_splits[target]\n",
        "\n",
        "        # Prepare text data using the tokenizer\n",
        "        if model_type == 'title':\n",
        "            texts_train = X_train['title'].values\n",
        "            texts_test = X_test['title'].values\n",
        "        else:\n",
        "            texts_train = X_train['text'].values\n",
        "            texts_test = X_test['text'].values\n",
        "\n",
        "        # Create DataLoader for training and testing\n",
        "        train_dataset = TextDataset(texts_train, y_train, tokenizer, config['max_len'])\n",
        "        test_dataset = TextDataset(texts_test, y_test, tokenizer, config['max_len'])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        # Model setup\n",
        "        num_labels = len(label_encoders[target].classes_)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=num_labels).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training process\n",
        "        model.train()\n",
        "        best_loss = float('inf')\n",
        "        early_stop_counter = 0\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            print(f\"Epoch {epoch+1}/{config['epochs']} - Training: {target}\")\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", total=len(train_loader), leave=True)\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            # Average loss for the epoch\n",
        "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"Average Training Loss for Epoch {epoch+1}: {avg_epoch_loss}\")\n",
        "\n",
        "            # Evaluate on the test set to compute validation loss\n",
        "            val_loss = 0.0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in test_loader:\n",
        "                    input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                    attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                    labels = batch['label'].to(device)\n",
        "\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(test_loader)\n",
        "            print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss}\")\n",
        "\n",
        "            # Step the scheduler with the validation loss\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                early_stop_counter = 0\n",
        "                torch.save(model.state_dict(), f\"best_model_{target}.pt\")  # Save the best model\n",
        "            else:\n",
        "                early_stop_counter += 1\n",
        "                print(f\"Early stopping counter: {early_stop_counter}/{early_stopping_patience}\")\n",
        "\n",
        "            if early_stop_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Load the best model for evaluation\n",
        "        model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n",
        "\n",
        "        # Evaluation process\n",
        "        print(f\"Evaluating model for task: {target}\")\n",
        "        model.eval()\n",
        "        y_preds = []\n",
        "        y_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\", total=len(test_loader), leave=True):\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, preds = torch.max(outputs.logits, dim=1)\n",
        "                y_preds.extend(preds.cpu().numpy())\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Decode labels back to original categories using the label encoder\n",
        "        decoded_preds = label_encoders[target].inverse_transform(y_preds)\n",
        "        decoded_true = label_encoders[target].inverse_transform(y_true)\n",
        "\n",
        "        # Calculate F1 score for the task\n",
        "        f1 = f1_score(decoded_true, decoded_preds, average='weighted')\n",
        "        f1_scores.append(f1)\n",
        "        print(f\"F1-Score for {target}: {f1}\")\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"Classification Report for {target}:\\n\")\n",
        "        print(classification_report(decoded_true, decoded_preds, zero_division=0))\n",
        "\n",
        "    return f1_scores  # Return the list of F1 scores for plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OYEB0MQmVAWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abef371-506f-4820-f9c5-e0ce2c97b200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Title Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.39it/s, loss=0.731]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 0.8391710806481697\n",
            "Validation Loss after Epoch 1: 0.684822328388691\n",
            "Epoch 2/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.0742]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.5030099867185304\n",
            "Validation Loss after Epoch 2: 0.5898981164209545\n",
            "Epoch 3/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.409]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.3212648796412599\n",
            "Validation Loss after Epoch 3: 0.6839247900061309\n",
            "Early stopping counter: 1/6\n",
            "Epoch 4/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.212170943227056\n",
            "Validation Loss after Epoch 4: 0.7780294450931251\n",
            "Early stopping counter: 2/6\n",
            "Epoch 5/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0884]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.1455102684171675\n",
            "Validation Loss after Epoch 5: 0.7540938910096884\n",
            "Early stopping counter: 3/6\n",
            "Epoch 6/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.1036541857561292\n",
            "Validation Loss after Epoch 6: 0.7857046816498041\n",
            "Early stopping counter: 4/6\n",
            "Epoch 7/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.104]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.061231792153842696\n",
            "Validation Loss after Epoch 7: 0.7417516740970314\n",
            "Early stopping counter: 5/6\n",
            "Epoch 8/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.00567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.04181492382618175\n",
            "Validation Loss after Epoch 8: 0.7439211782766506\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard-category: 0.82792968672088\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90       188\n",
            "           1       0.85      0.93      0.89       171\n",
            "           2       0.81      0.60      0.69        35\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.77      0.84      0.80        58\n",
            "           5       0.78      0.64      0.71        28\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.89      0.53      0.67        15\n",
            "           9       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.84       509\n",
            "   macro avg       0.50      0.45      0.47       509\n",
            "weighted avg       0.82      0.84      0.83       509\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.38it/s, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 1.7992793848881354\n",
            "Validation Loss after Epoch 1: 1.149048862978816\n",
            "Epoch 2/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.9122850159248272\n",
            "Validation Loss after Epoch 2: 0.9277553297579288\n",
            "Epoch 3/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0575]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.5123526228370366\n",
            "Validation Loss after Epoch 3: 0.9759407956153154\n",
            "Early stopping counter: 1/6\n",
            "Epoch 4/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.28581313383798085\n",
            "Validation Loss after Epoch 4: 1.0119524272158742\n",
            "Early stopping counter: 2/6\n",
            "Epoch 5/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.16936148974679774\n",
            "Validation Loss after Epoch 5: 1.1206264458596706\n",
            "Early stopping counter: 3/6\n",
            "Epoch 6/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.11000564838336273\n",
            "Validation Loss after Epoch 6: 1.2314193919301033\n",
            "Early stopping counter: 4/6\n",
            "Epoch 7/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0244]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.05184709920932155\n",
            "Validation Loss after Epoch 7: 1.194928282406181\n",
            "Early stopping counter: 5/6\n",
            "Epoch 8/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.00747]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.03526347557935942\n",
            "Validation Loss after Epoch 8: 1.2035068394616246\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product-category: 0.7263395818998133\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      1.00      0.80         2\n",
            "           1       0.61      0.81      0.70        54\n",
            "           2       0.75      0.83      0.79        29\n",
            "           3       0.67      0.20      0.31        20\n",
            "           4       0.41      0.64      0.50        11\n",
            "           5       0.50      0.33      0.40         3\n",
            "           6       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.78      0.80      0.79        56\n",
            "          10       0.44      0.44      0.44         9\n",
            "          12       0.97      0.88      0.92        33\n",
            "          13       0.82      0.95      0.88       131\n",
            "          14       0.87      0.81      0.84        16\n",
            "          15       0.78      0.84      0.81        37\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.57      0.24      0.33        51\n",
            "          19       0.96      0.89      0.93        28\n",
            "          20       0.65      0.81      0.72        21\n",
            "          21       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.75       509\n",
            "   macro avg       0.52      0.52      0.51       509\n",
            "weighted avg       0.73      0.75      0.73       509\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.38it/s, loss=3.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.8727127117710514\n",
            "Validation Loss after Epoch 1: 2.055177252739668\n",
            "Epoch 2/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=2.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.8492886586622759\n",
            "Validation Loss after Epoch 2: 1.7318320292979479\n",
            "Epoch 3/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.893]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 1.3893939102446282\n",
            "Validation Loss after Epoch 3: 1.6055078636854887\n",
            "Epoch 4/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.637]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 1.0161088964530638\n",
            "Validation Loss after Epoch 4: 1.5895892567932606\n",
            "Epoch 5/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=1.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.7316830027587973\n",
            "Validation Loss after Epoch 5: 1.6311842147260904\n",
            "Early stopping counter: 1/6\n",
            "Epoch 6/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.182]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.536114669528666\n",
            "Validation Loss after Epoch 6: 1.5707994624972343\n",
            "Epoch 7/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.457]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.3821847282897134\n",
            "Validation Loss after Epoch 7: 1.660772131755948\n",
            "Early stopping counter: 1/6\n",
            "Epoch 8/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.2686733654993293\n",
            "Validation Loss after Epoch 8: 1.6875372752547264\n",
            "Early stopping counter: 2/6\n",
            "Epoch 9/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.119]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.18680512819478248\n",
            "Validation Loss after Epoch 9: 1.6799706406891346\n",
            "Early stopping counter: 3/6\n",
            "Epoch 10/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.016]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.14128560437397522\n",
            "Validation Loss after Epoch 10: 1.7588777542114258\n",
            "Early stopping counter: 4/6\n",
            "Epoch 11/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.09252552018611582\n",
            "Validation Loss after Epoch 11: 1.7114620264619589\n",
            "Early stopping counter: 5/6\n",
            "Epoch 12/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.0894]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.07581622189829139\n",
            "Validation Loss after Epoch 12: 1.7032967396080494\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard: 0.6359264217601279\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         2\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.30      0.75      0.43         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         2\n",
            "          14       0.40      0.40      0.40         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       0.48      0.59      0.53        17\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          22       1.00      1.00      1.00         3\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          27       0.33      1.00      0.50         1\n",
            "          28       0.50      1.00      0.67         1\n",
            "          33       0.67      1.00      0.80         2\n",
            "          34       0.61      0.69      0.65        16\n",
            "          36       0.63      0.75      0.69        16\n",
            "          38       1.00      0.40      0.57         5\n",
            "          39       0.50      0.50      0.50         2\n",
            "          40       0.67      0.20      0.31        10\n",
            "          42       0.67      1.00      0.80         2\n",
            "          43       0.20      0.25      0.22         4\n",
            "          45       0.50      1.00      0.67         1\n",
            "          47       0.00      0.00      0.00         2\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      1.00      1.00         4\n",
            "          52       0.78      1.00      0.88         7\n",
            "          54       0.25      0.33      0.29         3\n",
            "          55       0.76      0.88      0.81        75\n",
            "          56       0.00      0.00      0.00         1\n",
            "          57       0.43      0.62      0.51        16\n",
            "          58       0.00      0.00      0.00         2\n",
            "          59       0.75      0.85      0.79        59\n",
            "          61       0.00      0.00      0.00         1\n",
            "          64       1.00      1.00      1.00         1\n",
            "          65       1.00      1.00      1.00         5\n",
            "          68       1.00      1.00      1.00         2\n",
            "          70       0.00      0.00      0.00         2\n",
            "          73       0.48      0.71      0.57        14\n",
            "          75       0.33      0.33      0.33         3\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       1.00      0.33      0.50         3\n",
            "          79       0.00      0.00      0.00         1\n",
            "          80       0.00      0.00      0.00         2\n",
            "          82       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          85       0.73      0.79      0.76        24\n",
            "          86       1.00      1.00      1.00         3\n",
            "          89       0.00      0.00      0.00         2\n",
            "          90       0.54      0.52      0.53        25\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         2\n",
            "          94       0.67      0.50      0.57         4\n",
            "          95       0.00      0.00      0.00         1\n",
            "          97       1.00      0.33      0.50         3\n",
            "          98       0.79      0.79      0.79        66\n",
            "          99       0.75      0.60      0.67         5\n",
            "         100       0.80      0.24      0.36        17\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.00      0.00      0.00         1\n",
            "         104       1.00      1.00      1.00         1\n",
            "         107       0.00      0.00      0.00         2\n",
            "         108       1.00      0.33      0.50         3\n",
            "         109       0.71      0.91      0.80        11\n",
            "         110       0.00      0.00      0.00         2\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       0.50      1.00      0.67         1\n",
            "         114       0.80      0.80      0.80         5\n",
            "         116       0.00      0.00      0.00         0\n",
            "         119       1.00      1.00      1.00         8\n",
            "         121       0.25      1.00      0.40         1\n",
            "         126       0.50      0.67      0.57         3\n",
            "\n",
            "    accuracy                           0.67       509\n",
            "   macro avg       0.40      0.41      0.38       509\n",
            "weighted avg       0.64      0.67      0.64       509\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.37it/s, loss=6.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 6.336150942982494\n",
            "Validation Loss after Epoch 1: 5.858946770429611\n",
            "Epoch 2/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=5.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 5.31364850230984\n",
            "Validation Loss after Epoch 2: 5.0575438886880875\n",
            "Epoch 3/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=5.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 4.367346717761113\n",
            "Validation Loss after Epoch 3: 4.550716556608677\n",
            "Epoch 4/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=3.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 3.591390492199184\n",
            "Validation Loss after Epoch 4: 4.177453272044659\n",
            "Epoch 5/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=3.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 2.926560097224229\n",
            "Validation Loss after Epoch 5: 3.9132177382707596\n",
            "Epoch 6/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=1.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 2.351120855007972\n",
            "Validation Loss after Epoch 6: 3.729815222322941\n",
            "Epoch 7/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=2.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 1.865076778651951\n",
            "Validation Loss after Epoch 7: 3.590485967695713\n",
            "Epoch 8/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=1.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 1.4840326442585126\n",
            "Validation Loss after Epoch 8: 3.4700793847441673\n",
            "Epoch 9/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 1.1671310571002793\n",
            "Validation Loss after Epoch 9: 3.4302622452378273\n",
            "Epoch 10/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.812]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.9232274226166985\n",
            "Validation Loss after Epoch 10: 3.3886151015758514\n",
            "Epoch 11/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=0.457]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.7360304694075684\n",
            "Validation Loss after Epoch 11: 3.367245577275753\n",
            "Epoch 12/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.5815050220572865\n",
            "Validation Loss after Epoch 12: 3.325780987739563\n",
            "Epoch 13/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.764]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.47527359827206683\n",
            "Validation Loss after Epoch 13: 3.366954382508993\n",
            "Early stopping counter: 1/6\n",
            "Epoch 14/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.524]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.3809935844314474\n",
            "Validation Loss after Epoch 14: 3.3358466029167175\n",
            "Early stopping counter: 2/6\n",
            "Epoch 15/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=0.147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 15: 0.30307016265090114\n",
            "Validation Loss after Epoch 15: 3.344312012195587\n",
            "Early stopping counter: 3/6\n",
            "Epoch 16/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.229]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 16: 0.23932039076006495\n",
            "Validation Loss after Epoch 16: 3.364182762801647\n",
            "Early stopping counter: 4/6\n",
            "Epoch 17/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 17: 0.18319651020130703\n",
            "Validation Loss after Epoch 17: 3.3618218451738358\n",
            "Early stopping counter: 5/6\n",
            "Epoch 18/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 18: 0.1685072622633168\n",
            "Validation Loss after Epoch 18: 3.36287335306406\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product: 0.4678291770016206\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         3\n",
            "           2       1.00      0.60      0.75         5\n",
            "           4       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         1\n",
            "          13       0.75      1.00      0.86         3\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       0.50      1.00      0.67         1\n",
            "          22       1.00      1.00      1.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         0\n",
            "          30       0.00      0.00      0.00         0\n",
            "          31       0.00      0.00      0.00         1\n",
            "          35       0.00      0.00      0.00         1\n",
            "          38       1.00      1.00      1.00         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         0\n",
            "          49       0.50      1.00      0.67         1\n",
            "          51       0.50      1.00      0.67         1\n",
            "          55       0.00      0.00      0.00         1\n",
            "          58       0.33      0.33      0.33         3\n",
            "          61       0.50      0.50      0.50         2\n",
            "          62       0.00      0.00      0.00         1\n",
            "          67       0.33      1.00      0.50         1\n",
            "          69       0.00      0.00      0.00         0\n",
            "          73       0.00      0.00      0.00         2\n",
            "          74       1.00      1.00      1.00         1\n",
            "          76       0.00      0.00      0.00         0\n",
            "          80       0.00      0.00      0.00         0\n",
            "          84       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         2\n",
            "          88       0.00      0.00      0.00         0\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         2\n",
            "          93       0.00      0.00      0.00         2\n",
            "         102       0.00      0.00      0.00         0\n",
            "         108       0.00      0.00      0.00         0\n",
            "         111       0.00      0.00      0.00         1\n",
            "         117       0.78      0.78      0.78         9\n",
            "         119       0.40      0.67      0.50         3\n",
            "         122       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         0\n",
            "         135       0.00      0.00      0.00         1\n",
            "         138       0.50      1.00      0.67         1\n",
            "         141       0.00      0.00      0.00         1\n",
            "         142       0.00      0.00      0.00         0\n",
            "         143       0.00      0.00      0.00         1\n",
            "         149       0.00      0.00      0.00         0\n",
            "         150       0.40      0.57      0.47         7\n",
            "         154       0.00      0.00      0.00         1\n",
            "         157       0.00      0.00      0.00         1\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       0.00      0.00      0.00         3\n",
            "         167       0.45      0.45      0.45        11\n",
            "         168       0.00      0.00      0.00         0\n",
            "         169       0.00      0.00      0.00         1\n",
            "         170       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         1\n",
            "         174       0.00      0.00      0.00         0\n",
            "         177       0.14      0.33      0.20         3\n",
            "         186       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         4\n",
            "         201       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         0\n",
            "         207       1.00      0.33      0.50         6\n",
            "         209       0.00      0.00      0.00         2\n",
            "         210       0.00      0.00      0.00         1\n",
            "         212       0.67      0.50      0.57         4\n",
            "         213       0.00      0.00      0.00         0\n",
            "         214       0.00      0.00      0.00         0\n",
            "         219       1.00      1.00      1.00         2\n",
            "         224       0.40      0.67      0.50         3\n",
            "         226       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         0\n",
            "         231       1.00      0.67      0.80         3\n",
            "         240       0.50      1.00      0.67         1\n",
            "         241       0.00      0.00      0.00         1\n",
            "         245       0.67      1.00      0.80         2\n",
            "         248       0.00      0.00      0.00         1\n",
            "         252       0.00      0.00      0.00         0\n",
            "         255       0.25      1.00      0.40         1\n",
            "         260       0.57      0.80      0.67         5\n",
            "         261       0.00      0.00      0.00         0\n",
            "         262       0.00      0.00      0.00         0\n",
            "         263       0.50      0.67      0.57         3\n",
            "         264       0.00      0.00      0.00         1\n",
            "         267       0.00      0.00      0.00         1\n",
            "         268       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         1\n",
            "         279       1.00      1.00      1.00         1\n",
            "         284       0.00      0.00      0.00         1\n",
            "         285       0.00      0.00      0.00         0\n",
            "         286       0.00      0.00      0.00         1\n",
            "         295       0.60      1.00      0.75         3\n",
            "         296       1.00      1.00      1.00         1\n",
            "         298       1.00      0.50      0.67         2\n",
            "         301       0.25      0.50      0.33         2\n",
            "         302       0.00      0.00      0.00         1\n",
            "         303       0.33      0.50      0.40         2\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       1.00      1.00      1.00         3\n",
            "         313       1.00      1.00      1.00         6\n",
            "         316       1.00      1.00      1.00         1\n",
            "         317       0.00      0.00      0.00         1\n",
            "         319       1.00      1.00      1.00         1\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.50      1.00      0.67         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         341       0.00      0.00      0.00         0\n",
            "         346       0.80      1.00      0.89         4\n",
            "         347       0.00      0.00      0.00         1\n",
            "         349       0.00      0.00      0.00         0\n",
            "         350       0.00      0.00      0.00         0\n",
            "         353       0.00      0.00      0.00         2\n",
            "         354       1.00      1.00      1.00         1\n",
            "         361       0.00      0.00      0.00         0\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00         1\n",
            "         372       1.00      0.50      0.67         2\n",
            "         373       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         0\n",
            "         378       0.00      0.00      0.00         1\n",
            "         381       0.00      0.00      0.00         0\n",
            "         386       0.33      1.00      0.50         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         1\n",
            "         396       0.00      0.00      0.00         2\n",
            "         397       0.00      0.00      0.00         0\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00         1\n",
            "         407       0.00      0.00      0.00         1\n",
            "         415       0.00      0.00      0.00         0\n",
            "         419       0.00      0.00      0.00         1\n",
            "         427       0.00      0.00      0.00         0\n",
            "         428       0.00      0.00      0.00         1\n",
            "         430       1.00      0.50      0.67         2\n",
            "         438       0.00      0.00      0.00         1\n",
            "         439       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         453       0.00      0.00      0.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         2\n",
            "         460       0.00      0.00      0.00         0\n",
            "         461       0.00      0.00      0.00         0\n",
            "         462       1.00      1.00      1.00         1\n",
            "         463       0.00      0.00      0.00         0\n",
            "         466       0.00      0.00      0.00         0\n",
            "         468       0.00      0.00      0.00         1\n",
            "         476       1.00      1.00      1.00         1\n",
            "         485       0.00      0.00      0.00         1\n",
            "         486       1.00      1.00      1.00         1\n",
            "         492       0.50      1.00      0.67         1\n",
            "         494       0.00      0.00      0.00         0\n",
            "         500       0.00      0.00      0.00         1\n",
            "         501       0.00      0.00      0.00         1\n",
            "         502       0.38      0.83      0.53         6\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         508       1.00      1.00      1.00         1\n",
            "         509       0.00      0.00      0.00         1\n",
            "         510       0.50      0.50      0.50         2\n",
            "         512       0.00      0.00      0.00         0\n",
            "         520       1.00      0.33      0.50         3\n",
            "         524       1.00      1.00      1.00         1\n",
            "         530       1.00      0.97      0.98        31\n",
            "         535       1.00      1.00      1.00         2\n",
            "         536       0.00      0.00      0.00         1\n",
            "         537       0.00      0.00      0.00         1\n",
            "         539       0.00      0.00      0.00         0\n",
            "         540       0.67      0.67      0.67         3\n",
            "         541       1.00      1.00      1.00         3\n",
            "         543       0.00      0.00      0.00         0\n",
            "         544       0.00      0.00      0.00         0\n",
            "         545       1.00      1.00      1.00         1\n",
            "         548       1.00      1.00      1.00         1\n",
            "         550       0.00      0.00      0.00         1\n",
            "         554       1.00      1.00      1.00         1\n",
            "         557       1.00      1.00      1.00         1\n",
            "         558       0.00      0.00      0.00         1\n",
            "         566       0.00      0.00      0.00         1\n",
            "         567       0.00      0.00      0.00         0\n",
            "         568       1.00      1.00      1.00         1\n",
            "         569       1.00      0.50      0.67         2\n",
            "         571       0.00      0.00      0.00         1\n",
            "         573       0.00      0.00      0.00         1\n",
            "         575       0.00      0.00      0.00         1\n",
            "         576       0.25      1.00      0.40         1\n",
            "         577       0.00      0.00      0.00         0\n",
            "         578       0.00      0.00      0.00         1\n",
            "         580       0.00      0.00      0.00         1\n",
            "         582       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         1\n",
            "         588       0.00      0.00      0.00         1\n",
            "         591       0.40      1.00      0.57         2\n",
            "         593       1.00      1.00      1.00         2\n",
            "         594       0.00      0.00      0.00         1\n",
            "         596       0.00      0.00      0.00         0\n",
            "         597       0.00      0.00      0.00         1\n",
            "         601       0.00      0.00      0.00         1\n",
            "         608       0.75      1.00      0.86         3\n",
            "         609       0.00      0.00      0.00         1\n",
            "         613       0.00      0.00      0.00         1\n",
            "         614       0.00      0.00      0.00         1\n",
            "         616       1.00      1.00      1.00         1\n",
            "         617       0.75      1.00      0.86         3\n",
            "         621       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         0\n",
            "         623       1.00      1.00      1.00         1\n",
            "         627       0.00      0.00      0.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         631       0.00      0.00      0.00         1\n",
            "         638       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         647       1.00      1.00      1.00         1\n",
            "         648       1.00      1.00      1.00         2\n",
            "         649       0.60      1.00      0.75         3\n",
            "         652       0.00      0.00      0.00         1\n",
            "         653       0.00      0.00      0.00         1\n",
            "         654       0.00      0.00      0.00         1\n",
            "         662       0.00      0.00      0.00         2\n",
            "         665       0.25      0.67      0.36         3\n",
            "         666       0.50      0.50      0.50         2\n",
            "         667       0.00      0.00      0.00         1\n",
            "         669       1.00      1.00      1.00         3\n",
            "         670       1.00      1.00      1.00         1\n",
            "         672       1.00      0.50      0.67         2\n",
            "         677       0.00      0.00      0.00         0\n",
            "         680       0.00      0.00      0.00         0\n",
            "         685       0.00      0.00      0.00         1\n",
            "         686       0.33      0.50      0.40         2\n",
            "         690       1.00      0.86      0.92         7\n",
            "         691       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         700       0.75      1.00      0.86         3\n",
            "         706       1.00      1.00      1.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.25      1.00      0.40         1\n",
            "         710       0.00      0.00      0.00         1\n",
            "         711       1.00      1.00      1.00         5\n",
            "         712       0.50      1.00      0.67         1\n",
            "         713       1.00      1.00      1.00         1\n",
            "         714       0.00      0.00      0.00         1\n",
            "         717       0.00      0.00      0.00         0\n",
            "         726       0.00      0.00      0.00         1\n",
            "         728       0.67      1.00      0.80         2\n",
            "         731       1.00      0.50      0.67         2\n",
            "         734       0.67      1.00      0.80         2\n",
            "         742       0.67      1.00      0.80         2\n",
            "         743       0.00      0.00      0.00         0\n",
            "         744       0.00      0.00      0.00         0\n",
            "         748       0.00      0.00      0.00         0\n",
            "         749       0.00      0.00      0.00         3\n",
            "         755       0.00      0.00      0.00         1\n",
            "         756       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         0\n",
            "         762       0.00      0.00      0.00         1\n",
            "         764       1.00      1.00      1.00         1\n",
            "         770       1.00      1.00      1.00         2\n",
            "         772       0.00      0.00      0.00         1\n",
            "         774       0.00      0.00      0.00         1\n",
            "         775       1.00      1.00      1.00         1\n",
            "         776       1.00      0.33      0.50         3\n",
            "         778       0.00      0.00      0.00         1\n",
            "         781       0.00      0.00      0.00         1\n",
            "         782       0.08      0.10      0.09        10\n",
            "         786       0.00      0.00      0.00         1\n",
            "         788       0.00      0.00      0.00         1\n",
            "         789       0.00      0.00      0.00         1\n",
            "         796       0.00      0.00      0.00         1\n",
            "         798       0.00      0.00      0.00         0\n",
            "         809       0.00      0.00      0.00         1\n",
            "         810       0.50      0.67      0.57         6\n",
            "         811       1.00      0.50      0.67         2\n",
            "         814       0.75      0.75      0.75         4\n",
            "         815       0.00      0.00      0.00         1\n",
            "         816       0.00      0.00      0.00         0\n",
            "         820       0.83      0.83      0.83         6\n",
            "         822       0.00      0.00      0.00         1\n",
            "         825       0.25      0.20      0.22         5\n",
            "         826       0.00      0.00      0.00         0\n",
            "         829       0.00      0.00      0.00         1\n",
            "         831       0.00      0.00      0.00         0\n",
            "         832       0.00      0.00      0.00         1\n",
            "         833       0.33      1.00      0.50         1\n",
            "         835       0.00      0.00      0.00         1\n",
            "         837       0.00      0.00      0.00         1\n",
            "         846       0.00      0.00      0.00         0\n",
            "         847       0.00      0.00      0.00         0\n",
            "         848       0.00      0.00      0.00         1\n",
            "         849       0.50      1.00      0.67         1\n",
            "         854       0.67      1.00      0.80         2\n",
            "         858       0.00      0.00      0.00         1\n",
            "         860       1.00      1.00      1.00         1\n",
            "         861       0.50      0.25      0.33         4\n",
            "         863       0.00      0.00      0.00         2\n",
            "         870       0.50      1.00      0.67         3\n",
            "         873       0.00      0.00      0.00         1\n",
            "         885       0.00      0.00      0.00         0\n",
            "         887       0.00      0.00      0.00         1\n",
            "         892       0.00      0.00      0.00         0\n",
            "         894       0.00      0.00      0.00         0\n",
            "         895       1.00      1.00      1.00         1\n",
            "         898       1.00      1.00      1.00         2\n",
            "         899       0.00      0.00      0.00         1\n",
            "         903       0.00      0.00      0.00         0\n",
            "         908       1.00      1.00      1.00         1\n",
            "         912       0.67      0.80      0.73         5\n",
            "         916       0.00      0.00      0.00         1\n",
            "         921       0.00      0.00      0.00         1\n",
            "         922       0.75      0.60      0.67         5\n",
            "         923       0.00      0.00      0.00         1\n",
            "         928       0.00      0.00      0.00         0\n",
            "         929       0.00      0.00      0.00         1\n",
            "         930       0.00      0.00      0.00         1\n",
            "         932       0.33      0.50      0.40         2\n",
            "         933       0.40      1.00      0.57         2\n",
            "         945       0.00      0.00      0.00         0\n",
            "         946       1.00      1.00      1.00         3\n",
            "         950       0.00      0.00      0.00         1\n",
            "         951       0.00      0.00      0.00         2\n",
            "         952       0.00      0.00      0.00         0\n",
            "         955       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         971       0.00      0.00      0.00         1\n",
            "         973       0.00      0.00      0.00         0\n",
            "         982       0.00      0.00      0.00         1\n",
            "         992       0.00      0.00      0.00         1\n",
            "         998       1.00      1.00      1.00         2\n",
            "         999       1.00      1.00      1.00         1\n",
            "        1007       0.50      1.00      0.67         1\n",
            "        1013       0.00      0.00      0.00         1\n",
            "        1017       1.00      1.00      1.00         1\n",
            "        1018       0.00      0.00      0.00         2\n",
            "        1020       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.50       509\n",
            "   macro avg       0.26      0.29      0.27       509\n",
            "weighted avg       0.47      0.50      0.47       509\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate for both title and text\n",
        "print(\"\\nTraining and Evaluating for Title Tasks:\")\n",
        "title_f1_scores = train_and_evaluate_nn(title_splits, subtask1 + subtask2, model_type='title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsSoAzzgOyct",
        "outputId": "ede79e68-c0d3-4dfd-8cc3-74b7980e8aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Text Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.97it/s, loss=0.599]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 0.6290615882880829\n",
            "Validation Loss after Epoch 1: 0.3245128197595477\n",
            "Epoch 2/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.186]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.25798709008035126\n",
            "Validation Loss after Epoch 2: 0.2419404195388779\n",
            "Epoch 3/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0574]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.17730917175092012\n",
            "Validation Loss after Epoch 3: 0.22245623762137257\n",
            "Epoch 4/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.13641293596645648\n",
            "Validation Loss after Epoch 4: 0.21702212584204972\n",
            "Epoch 5/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.1009764209734862\n",
            "Validation Loss after Epoch 5: 0.23921011027414352\n",
            "Early stopping counter: 1/6\n",
            "Epoch 6/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.0262]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.10650748891554043\n",
            "Validation Loss after Epoch 6: 0.2276633754663635\n",
            "Early stopping counter: 2/6\n",
            "Epoch 7/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.11438136742453553\n",
            "Validation Loss after Epoch 7: 0.22027365372923668\n",
            "Early stopping counter: 3/6\n",
            "Epoch 8/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0308]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.08077404431923833\n",
            "Validation Loss after Epoch 8: 0.21204899798613042\n",
            "Epoch 9/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.00206]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.047193714117523856\n",
            "Validation Loss after Epoch 9: 0.26783676442937576\n",
            "Early stopping counter: 1/6\n",
            "Epoch 10/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:34<00:00,  3.02it/s, loss=0.00276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.03686159319741736\n",
            "Validation Loss after Epoch 10: 0.2671974256809335\n",
            "Early stopping counter: 2/6\n",
            "Epoch 11/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.00962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.03482082267548283\n",
            "Validation Loss after Epoch 11: 0.2514030852835276\n",
            "Early stopping counter: 3/6\n",
            "Epoch 12/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.00362]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.038772959444693035\n",
            "Validation Loss after Epoch 12: 0.2742101990879746\n",
            "Early stopping counter: 4/6\n",
            "Epoch 13/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.00635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.01418877428542445\n",
            "Validation Loss after Epoch 13: 0.2723349534862791\n",
            "Early stopping counter: 5/6\n",
            "Epoch 14/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.00191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.01009624600703535\n",
            "Validation Loss after Epoch 14: 0.27530865275184624\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard-category: 0.9417192745216745\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       188\n",
            "           1       0.98      0.99      0.99       171\n",
            "           2       0.92      1.00      0.96        35\n",
            "           3       0.50      0.20      0.29         5\n",
            "           4       0.98      1.00      0.99        58\n",
            "           5       0.69      0.79      0.73        28\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.60      1.00      0.75         3\n",
            "           8       0.90      0.60      0.72        15\n",
            "           9       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           0.94       509\n",
            "   macro avg       0.75      0.71      0.71       509\n",
            "weighted avg       0.94      0.94      0.94       509\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.98it/s, loss=2.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.165630628595819\n",
            "Validation Loss after Epoch 1: 1.860517218708992\n",
            "Epoch 2/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:34<00:00,  3.02it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.3998593020064014\n",
            "Validation Loss after Epoch 2: 1.3338965829461813\n",
            "Epoch 3/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=1.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.9339450276814975\n",
            "Validation Loss after Epoch 3: 1.0170307783409953\n",
            "Epoch 4/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.266]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.626533744505027\n",
            "Validation Loss after Epoch 4: 0.9829536005854607\n",
            "Epoch 5/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=1.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.43524510621816126\n",
            "Validation Loss after Epoch 5: 1.05213917279616\n",
            "Early stopping counter: 1/6\n",
            "Epoch 6/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.30570460863124865\n",
            "Validation Loss after Epoch 6: 1.21366445440799\n",
            "Early stopping counter: 2/6\n",
            "Epoch 7/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.243]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.21236594111521864\n",
            "Validation Loss after Epoch 7: 1.2107336707413197\n",
            "Early stopping counter: 3/6\n",
            "Epoch 8/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.0126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.14322448491568227\n",
            "Validation Loss after Epoch 8: 1.3553482294082642\n",
            "Early stopping counter: 4/6\n",
            "Epoch 9/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.00882]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.09298340198779283\n",
            "Validation Loss after Epoch 9: 1.2641982247587293\n",
            "Early stopping counter: 5/6\n",
            "Epoch 10/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.06074646849452251\n",
            "Validation Loss after Epoch 10: 1.2643121511209756\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product-category: 0.7221893294931326\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       0.73      0.67      0.70        54\n",
            "           2       0.69      0.76      0.72        29\n",
            "           3       0.35      0.40      0.37        20\n",
            "           4       0.88      0.64      0.74        11\n",
            "           5       1.00      0.33      0.50         3\n",
            "           6       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.79      0.79      0.79        56\n",
            "          10       0.30      0.67      0.41         9\n",
            "          12       0.97      0.91      0.94        33\n",
            "          13       0.86      0.88      0.87       131\n",
            "          14       1.00      0.81      0.90        16\n",
            "          15       0.74      0.54      0.62        37\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.75      1.00      0.86         3\n",
            "          18       0.45      0.49      0.47        51\n",
            "          19       0.90      0.64      0.75        28\n",
            "          20       0.47      0.76      0.58        21\n",
            "          21       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.72       509\n",
            "   macro avg       0.59      0.56      0.56       509\n",
            "weighted avg       0.74      0.72      0.72       509\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.97it/s, loss=1.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.69183080419377\n",
            "Validation Loss after Epoch 1: 1.464539966545999\n",
            "Epoch 2/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=1.53]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.2450363259840678\n",
            "Validation Loss after Epoch 2: 0.9808227133471519\n",
            "Epoch 3/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.737]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.8678210198983446\n",
            "Validation Loss after Epoch 3: 0.7922879987163469\n",
            "Epoch 4/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.203]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.6374109331388157\n",
            "Validation Loss after Epoch 4: 0.7032566380221397\n",
            "Epoch 5/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.518]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.48657346523360356\n",
            "Validation Loss after Epoch 5: 0.7177542063873261\n",
            "Early stopping counter: 1/6\n",
            "Epoch 6/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.38531388214730716\n",
            "Validation Loss after Epoch 6: 0.6464546208735555\n",
            "Epoch 7/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.332]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.29670669366377633\n",
            "Validation Loss after Epoch 7: 0.6383900495711714\n",
            "Epoch 8/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.22553408688416224\n",
            "Validation Loss after Epoch 8: 0.6486862966557965\n",
            "Early stopping counter: 1/6\n",
            "Epoch 9/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0304]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.1636092308778446\n",
            "Validation Loss after Epoch 9: 0.6387864230782725\n",
            "Early stopping counter: 2/6\n",
            "Epoch 10/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0941]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.11838461182542614\n",
            "Validation Loss after Epoch 10: 0.6703560353198554\n",
            "Early stopping counter: 3/6\n",
            "Epoch 11/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.301]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.095334633106978\n",
            "Validation Loss after Epoch 11: 0.7027192693785764\n",
            "Early stopping counter: 4/6\n",
            "Epoch 12/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0152]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.06393971808183078\n",
            "Validation Loss after Epoch 12: 0.6931241928832605\n",
            "Early stopping counter: 5/6\n",
            "Epoch 13/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0163]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.04876733647327271\n",
            "Validation Loss after Epoch 13: 0.6935719783214154\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard: 0.8500440569622675\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.80      1.00      0.89         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       1.00      1.00      1.00         1\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      0.50      0.67         2\n",
            "          14       1.00      0.80      0.89         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       0.77      1.00      0.87        17\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.50      1.00      0.67         1\n",
            "          22       1.00      1.00      1.00         3\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       1.00      1.00      1.00         1\n",
            "          33       1.00      0.50      0.67         2\n",
            "          34       0.93      0.88      0.90        16\n",
            "          36       1.00      1.00      1.00        16\n",
            "          38       1.00      1.00      1.00         5\n",
            "          39       0.50      0.50      0.50         2\n",
            "          40       0.91      1.00      0.95        10\n",
            "          42       1.00      1.00      1.00         2\n",
            "          43       0.80      1.00      0.89         4\n",
            "          45       0.00      0.00      0.00         1\n",
            "          47       0.00      0.00      0.00         2\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      1.00      1.00         4\n",
            "          52       0.70      1.00      0.82         7\n",
            "          53       0.00      0.00      0.00         0\n",
            "          54       0.29      0.67      0.40         3\n",
            "          55       0.97      1.00      0.99        75\n",
            "          56       0.00      0.00      0.00         1\n",
            "          57       0.94      0.94      0.94        16\n",
            "          58       0.50      0.50      0.50         2\n",
            "          59       0.95      0.97      0.96        59\n",
            "          61       0.00      0.00      0.00         1\n",
            "          64       0.50      1.00      0.67         1\n",
            "          65       1.00      1.00      1.00         5\n",
            "          68       1.00      1.00      1.00         2\n",
            "          69       0.00      0.00      0.00         0\n",
            "          70       0.17      0.50      0.25         2\n",
            "          73       0.47      0.50      0.48        14\n",
            "          75       0.50      0.67      0.57         3\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       1.00      0.67      0.80         3\n",
            "          79       0.00      0.00      0.00         1\n",
            "          80       0.00      0.00      0.00         2\n",
            "          82       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          85       0.88      0.92      0.90        24\n",
            "          86       1.00      1.00      1.00         3\n",
            "          89       1.00      0.50      0.67         2\n",
            "          90       0.96      0.96      0.96        25\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       1.00      0.50      0.67         2\n",
            "          94       0.67      1.00      0.80         4\n",
            "          95       0.00      0.00      0.00         1\n",
            "          97       0.67      0.67      0.67         3\n",
            "          98       0.98      0.95      0.97        66\n",
            "          99       1.00      1.00      1.00         5\n",
            "         100       0.94      0.88      0.91        17\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.33      1.00      0.50         1\n",
            "         104       1.00      1.00      1.00         1\n",
            "         105       0.00      0.00      0.00         0\n",
            "         107       0.00      0.00      0.00         2\n",
            "         108       1.00      0.67      0.80         3\n",
            "         109       0.73      1.00      0.85        11\n",
            "         110       0.00      0.00      0.00         2\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       1.00      1.00      1.00         1\n",
            "         114       1.00      1.00      1.00         5\n",
            "         118       0.00      0.00      0.00         0\n",
            "         119       1.00      1.00      1.00         8\n",
            "         120       0.00      0.00      0.00         0\n",
            "         121       0.00      0.00      0.00         1\n",
            "         126       1.00      0.67      0.80         3\n",
            "         127       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.86       509\n",
            "   macro avg       0.50      0.52      0.50       509\n",
            "weighted avg       0.85      0.86      0.85       509\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.98it/s, loss=6.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 6.341293988527951\n",
            "Validation Loss after Epoch 1: 5.876046866178513\n",
            "Epoch 2/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=3.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 5.338552538331572\n",
            "Validation Loss after Epoch 2: 5.083326235413551\n",
            "Epoch 3/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=4.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 4.426661391358276\n",
            "Validation Loss after Epoch 3: 4.601574383676052\n",
            "Epoch 4/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=4.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 3.6530434526763598\n",
            "Validation Loss after Epoch 4: 4.230145379900932\n",
            "Epoch 5/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=3.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 2.97862971120781\n",
            "Validation Loss after Epoch 5: 3.8918039351701736\n",
            "Epoch 6/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=2.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 2.4054010093628944\n",
            "Validation Loss after Epoch 6: 3.7032704800367355\n",
            "Epoch 7/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 1.906824576062756\n",
            "Validation Loss after Epoch 7: 3.569550931453705\n",
            "Epoch 8/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=1.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 1.5080041301834\n",
            "Validation Loss after Epoch 8: 3.499046929180622\n",
            "Epoch 9/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.943]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 1.1763537681915543\n",
            "Validation Loss after Epoch 9: 3.4084729105234146\n",
            "Epoch 10/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.78]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.9211796951460671\n",
            "Validation Loss after Epoch 10: 3.3559048399329185\n",
            "Epoch 11/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.627]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.7241443922886481\n",
            "Validation Loss after Epoch 11: 3.333665654063225\n",
            "Epoch 12/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.5708555967084594\n",
            "Validation Loss after Epoch 12: 3.324141375720501\n",
            "Epoch 13/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.164]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.45136316975096724\n",
            "Validation Loss after Epoch 13: 3.3047217205166817\n",
            "Epoch 14/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.277]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.35751560227489554\n",
            "Validation Loss after Epoch 14: 3.35725224763155\n",
            "Early stopping counter: 1/6\n",
            "Epoch 15/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.574]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 15: 0.2825933822228775\n",
            "Validation Loss after Epoch 15: 3.3241336792707443\n",
            "Early stopping counter: 2/6\n",
            "Epoch 16/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.303]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 16: 0.22404296279370367\n",
            "Validation Loss after Epoch 16: 3.3500266522169113\n",
            "Early stopping counter: 3/6\n",
            "Epoch 17/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0301]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 17: 0.17322348949818553\n",
            "Validation Loss after Epoch 17: 3.3947231397032738\n",
            "Early stopping counter: 4/6\n",
            "Epoch 18/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 286/286 [01:35<00:00,  2.99it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 18: 0.13276964594069476\n",
            "Validation Loss after Epoch 18: 3.3838233798742294\n",
            "Early stopping counter: 5/6\n",
            "Epoch 19/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19: 100%|██████████| 286/286 [01:35<00:00,  2.99it/s, loss=0.0709]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 19: 0.12142692039349487\n",
            "Validation Loss after Epoch 19: 3.3809075579047203\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product: 0.4647634090165141\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           2       0.75      0.60      0.67         5\n",
            "           4       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          13       1.00      0.67      0.80         3\n",
            "          19       0.67      1.00      0.80         2\n",
            "          20       0.33      1.00      0.50         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          31       0.00      0.00      0.00         1\n",
            "          35       0.00      0.00      0.00         1\n",
            "          38       1.00      1.00      1.00         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         0\n",
            "          49       1.00      1.00      1.00         1\n",
            "          51       0.50      1.00      0.67         1\n",
            "          53       0.00      0.00      0.00         0\n",
            "          55       0.00      0.00      0.00         1\n",
            "          58       0.50      0.33      0.40         3\n",
            "          61       0.33      0.50      0.40         2\n",
            "          62       0.00      0.00      0.00         1\n",
            "          67       0.50      1.00      0.67         1\n",
            "          69       0.00      0.00      0.00         0\n",
            "          73       0.00      0.00      0.00         2\n",
            "          74       0.50      1.00      0.67         1\n",
            "          76       0.00      0.00      0.00         0\n",
            "          80       0.00      0.00      0.00         0\n",
            "          84       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         2\n",
            "          88       0.00      0.00      0.00         0\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         2\n",
            "          93       0.00      0.00      0.00         2\n",
            "          96       0.00      0.00      0.00         0\n",
            "         102       0.00      0.00      0.00         0\n",
            "         108       0.00      0.00      0.00         0\n",
            "         111       0.00      0.00      0.00         1\n",
            "         117       0.55      0.67      0.60         9\n",
            "         119       0.25      0.33      0.29         3\n",
            "         122       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         0\n",
            "         135       0.00      0.00      0.00         1\n",
            "         136       0.00      0.00      0.00         0\n",
            "         138       1.00      1.00      1.00         1\n",
            "         141       0.00      0.00      0.00         1\n",
            "         142       0.00      0.00      0.00         0\n",
            "         143       0.00      0.00      0.00         1\n",
            "         144       0.00      0.00      0.00         0\n",
            "         149       0.00      0.00      0.00         0\n",
            "         150       0.56      0.71      0.62         7\n",
            "         154       0.00      0.00      0.00         1\n",
            "         157       0.00      0.00      0.00         1\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       0.00      0.00      0.00         3\n",
            "         167       0.57      0.73      0.64        11\n",
            "         168       0.00      0.00      0.00         0\n",
            "         169       0.00      0.00      0.00         1\n",
            "         170       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         1\n",
            "         176       0.00      0.00      0.00         0\n",
            "         177       0.20      0.33      0.25         3\n",
            "         178       0.00      0.00      0.00         0\n",
            "         186       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         4\n",
            "         201       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         0\n",
            "         204       0.00      0.00      0.00         0\n",
            "         207       0.33      0.17      0.22         6\n",
            "         209       0.00      0.00      0.00         2\n",
            "         210       1.00      1.00      1.00         1\n",
            "         212       0.50      0.75      0.60         4\n",
            "         213       0.00      0.00      0.00         0\n",
            "         219       0.50      0.50      0.50         2\n",
            "         224       0.67      0.67      0.67         3\n",
            "         226       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         0\n",
            "         231       1.00      0.67      0.80         3\n",
            "         240       1.00      1.00      1.00         1\n",
            "         241       0.00      0.00      0.00         1\n",
            "         243       0.00      0.00      0.00         0\n",
            "         245       1.00      1.00      1.00         2\n",
            "         248       0.00      0.00      0.00         1\n",
            "         252       0.00      0.00      0.00         0\n",
            "         253       0.00      0.00      0.00         0\n",
            "         255       1.00      1.00      1.00         1\n",
            "         260       0.80      0.80      0.80         5\n",
            "         263       0.40      0.67      0.50         3\n",
            "         264       0.00      0.00      0.00         1\n",
            "         267       0.00      0.00      0.00         1\n",
            "         268       0.00      0.00      0.00         1\n",
            "         269       0.00      0.00      0.00         0\n",
            "         273       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         1\n",
            "         279       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00         1\n",
            "         286       0.00      0.00      0.00         1\n",
            "         294       0.00      0.00      0.00         0\n",
            "         295       0.50      1.00      0.67         3\n",
            "         296       1.00      1.00      1.00         1\n",
            "         298       0.50      0.50      0.50         2\n",
            "         301       0.14      0.50      0.22         2\n",
            "         302       0.00      0.00      0.00         1\n",
            "         303       0.50      0.50      0.50         2\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       0.75      1.00      0.86         3\n",
            "         313       0.86      1.00      0.92         6\n",
            "         316       1.00      1.00      1.00         1\n",
            "         317       0.00      0.00      0.00         1\n",
            "         319       1.00      1.00      1.00         1\n",
            "         327       0.00      0.00      0.00         0\n",
            "         329       0.00      0.00      0.00         0\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.50      1.00      0.67         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         346       0.80      1.00      0.89         4\n",
            "         347       0.00      0.00      0.00         1\n",
            "         353       0.00      0.00      0.00         2\n",
            "         354       1.00      1.00      1.00         1\n",
            "         361       0.00      0.00      0.00         0\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00         1\n",
            "         372       0.50      0.50      0.50         2\n",
            "         373       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         0\n",
            "         378       0.00      0.00      0.00         1\n",
            "         386       0.50      1.00      0.67         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         1\n",
            "         396       1.00      0.50      0.67         2\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00         1\n",
            "         407       0.00      0.00      0.00         1\n",
            "         419       0.00      0.00      0.00         1\n",
            "         427       0.00      0.00      0.00         0\n",
            "         428       0.00      0.00      0.00         1\n",
            "         430       0.00      0.00      0.00         2\n",
            "         432       0.00      0.00      0.00         0\n",
            "         438       0.00      0.00      0.00         1\n",
            "         439       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         450       0.00      0.00      0.00         0\n",
            "         453       1.00      1.00      1.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         458       1.00      0.50      0.67         2\n",
            "         460       0.00      0.00      0.00         0\n",
            "         461       0.00      0.00      0.00         0\n",
            "         462       1.00      1.00      1.00         1\n",
            "         466       0.00      0.00      0.00         0\n",
            "         468       1.00      1.00      1.00         1\n",
            "         476       0.50      1.00      0.67         1\n",
            "         485       0.00      0.00      0.00         1\n",
            "         486       1.00      1.00      1.00         1\n",
            "         491       0.00      0.00      0.00         0\n",
            "         492       0.00      0.00      0.00         1\n",
            "         494       0.00      0.00      0.00         0\n",
            "         500       0.00      0.00      0.00         1\n",
            "         501       0.00      0.00      0.00         1\n",
            "         502       0.75      1.00      0.86         6\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         508       1.00      1.00      1.00         1\n",
            "         509       0.00      0.00      0.00         1\n",
            "         510       0.33      0.50      0.40         2\n",
            "         512       0.00      0.00      0.00         0\n",
            "         513       0.00      0.00      0.00         0\n",
            "         520       0.00      0.00      0.00         3\n",
            "         522       0.00      0.00      0.00         0\n",
            "         524       1.00      1.00      1.00         1\n",
            "         530       1.00      0.90      0.95        31\n",
            "         535       1.00      1.00      1.00         2\n",
            "         536       0.00      0.00      0.00         1\n",
            "         537       0.00      0.00      0.00         1\n",
            "         540       0.67      0.67      0.67         3\n",
            "         541       0.75      1.00      0.86         3\n",
            "         543       0.00      0.00      0.00         0\n",
            "         545       1.00      1.00      1.00         1\n",
            "         547       0.00      0.00      0.00         0\n",
            "         548       1.00      1.00      1.00         1\n",
            "         550       0.00      0.00      0.00         1\n",
            "         554       1.00      1.00      1.00         1\n",
            "         557       1.00      1.00      1.00         1\n",
            "         558       0.00      0.00      0.00         1\n",
            "         566       0.00      0.00      0.00         1\n",
            "         568       0.33      1.00      0.50         1\n",
            "         569       1.00      0.50      0.67         2\n",
            "         571       0.00      0.00      0.00         1\n",
            "         573       0.00      0.00      0.00         1\n",
            "         575       0.00      0.00      0.00         1\n",
            "         576       0.50      1.00      0.67         1\n",
            "         577       0.00      0.00      0.00         0\n",
            "         578       0.00      0.00      0.00         1\n",
            "         580       0.00      0.00      0.00         1\n",
            "         582       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         1\n",
            "         588       0.00      0.00      0.00         1\n",
            "         591       0.40      1.00      0.57         2\n",
            "         593       1.00      1.00      1.00         2\n",
            "         594       0.00      0.00      0.00         1\n",
            "         596       0.00      0.00      0.00         0\n",
            "         597       0.00      0.00      0.00         1\n",
            "         598       0.00      0.00      0.00         0\n",
            "         601       1.00      1.00      1.00         1\n",
            "         608       1.00      0.67      0.80         3\n",
            "         609       0.00      0.00      0.00         1\n",
            "         613       0.00      0.00      0.00         1\n",
            "         614       0.00      0.00      0.00         1\n",
            "         615       0.00      0.00      0.00         0\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       1.00      0.67      0.80         3\n",
            "         618       0.00      0.00      0.00         0\n",
            "         621       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         0\n",
            "         623       1.00      1.00      1.00         1\n",
            "         627       0.00      0.00      0.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         631       0.00      0.00      0.00         1\n",
            "         638       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         647       0.50      1.00      0.67         1\n",
            "         648       1.00      1.00      1.00         2\n",
            "         649       0.67      0.67      0.67         3\n",
            "         652       0.00      0.00      0.00         1\n",
            "         653       0.00      0.00      0.00         1\n",
            "         654       0.00      0.00      0.00         1\n",
            "         662       0.00      0.00      0.00         2\n",
            "         665       0.17      0.33      0.22         3\n",
            "         666       0.50      0.50      0.50         2\n",
            "         667       1.00      1.00      1.00         1\n",
            "         669       1.00      1.00      1.00         3\n",
            "         670       1.00      1.00      1.00         1\n",
            "         672       1.00      0.50      0.67         2\n",
            "         679       0.00      0.00      0.00         0\n",
            "         685       0.00      0.00      0.00         1\n",
            "         686       0.25      0.50      0.33         2\n",
            "         690       0.83      0.71      0.77         7\n",
            "         691       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         700       0.75      1.00      0.86         3\n",
            "         706       1.00      1.00      1.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.00      0.00      0.00         1\n",
            "         710       0.00      0.00      0.00         1\n",
            "         711       1.00      1.00      1.00         5\n",
            "         712       0.00      0.00      0.00         1\n",
            "         713       0.33      1.00      0.50         1\n",
            "         714       0.00      0.00      0.00         1\n",
            "         726       0.00      0.00      0.00         1\n",
            "         727       0.00      0.00      0.00         0\n",
            "         728       0.67      1.00      0.80         2\n",
            "         731       0.00      0.00      0.00         2\n",
            "         734       0.50      1.00      0.67         2\n",
            "         742       1.00      0.50      0.67         2\n",
            "         749       0.00      0.00      0.00         3\n",
            "         755       0.00      0.00      0.00         1\n",
            "         756       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         0\n",
            "         762       0.00      0.00      0.00         1\n",
            "         764       0.00      0.00      0.00         1\n",
            "         770       1.00      1.00      1.00         2\n",
            "         772       0.00      0.00      0.00         1\n",
            "         774       0.00      0.00      0.00         1\n",
            "         775       1.00      1.00      1.00         1\n",
            "         776       0.00      0.00      0.00         3\n",
            "         778       0.00      0.00      0.00         1\n",
            "         781       0.00      0.00      0.00         1\n",
            "         782       0.33      0.40      0.36        10\n",
            "         786       0.00      0.00      0.00         1\n",
            "         788       0.00      0.00      0.00         1\n",
            "         789       1.00      1.00      1.00         1\n",
            "         791       0.00      0.00      0.00         0\n",
            "         793       0.00      0.00      0.00         0\n",
            "         796       0.00      0.00      0.00         1\n",
            "         798       0.00      0.00      0.00         0\n",
            "         809       0.00      0.00      0.00         1\n",
            "         810       0.57      0.67      0.62         6\n",
            "         811       1.00      1.00      1.00         2\n",
            "         814       1.00      0.75      0.86         4\n",
            "         815       0.00      0.00      0.00         1\n",
            "         820       1.00      0.83      0.91         6\n",
            "         822       0.00      0.00      0.00         1\n",
            "         825       0.00      0.00      0.00         5\n",
            "         826       0.00      0.00      0.00         0\n",
            "         829       0.00      0.00      0.00         1\n",
            "         831       0.00      0.00      0.00         0\n",
            "         832       0.00      0.00      0.00         1\n",
            "         833       0.50      1.00      0.67         1\n",
            "         835       0.00      0.00      0.00         1\n",
            "         837       0.00      0.00      0.00         1\n",
            "         844       0.00      0.00      0.00         0\n",
            "         847       0.00      0.00      0.00         0\n",
            "         848       0.00      0.00      0.00         1\n",
            "         849       0.50      1.00      0.67         1\n",
            "         854       0.67      1.00      0.80         2\n",
            "         858       0.50      1.00      0.67         1\n",
            "         860       0.00      0.00      0.00         1\n",
            "         861       1.00      0.25      0.40         4\n",
            "         863       0.00      0.00      0.00         2\n",
            "         870       0.38      1.00      0.55         3\n",
            "         873       0.00      0.00      0.00         1\n",
            "         885       0.00      0.00      0.00         0\n",
            "         887       0.00      0.00      0.00         1\n",
            "         895       1.00      1.00      1.00         1\n",
            "         898       0.50      1.00      0.67         2\n",
            "         899       0.00      0.00      0.00         1\n",
            "         903       0.00      0.00      0.00         0\n",
            "         908       0.50      1.00      0.67         1\n",
            "         912       0.67      0.80      0.73         5\n",
            "         916       0.00      0.00      0.00         1\n",
            "         921       0.00      0.00      0.00         1\n",
            "         922       1.00      0.80      0.89         5\n",
            "         923       0.00      0.00      0.00         1\n",
            "         928       0.00      0.00      0.00         0\n",
            "         929       0.00      0.00      0.00         1\n",
            "         930       0.00      0.00      0.00         1\n",
            "         932       0.00      0.00      0.00         2\n",
            "         933       0.67      1.00      0.80         2\n",
            "         945       0.00      0.00      0.00         0\n",
            "         946       1.00      0.67      0.80         3\n",
            "         950       0.33      1.00      0.50         1\n",
            "         951       0.00      0.00      0.00         2\n",
            "         952       0.00      0.00      0.00         0\n",
            "         955       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         966       0.00      0.00      0.00         0\n",
            "         971       0.00      0.00      0.00         1\n",
            "         973       0.00      0.00      0.00         0\n",
            "         978       0.00      0.00      0.00         0\n",
            "         982       0.00      0.00      0.00         1\n",
            "         987       0.00      0.00      0.00         0\n",
            "         992       0.00      0.00      0.00         1\n",
            "         996       0.00      0.00      0.00         0\n",
            "         997       0.00      0.00      0.00         0\n",
            "         998       1.00      1.00      1.00         2\n",
            "         999       1.00      1.00      1.00         1\n",
            "        1007       0.33      1.00      0.50         1\n",
            "        1013       1.00      1.00      1.00         1\n",
            "        1017       1.00      1.00      1.00         1\n",
            "        1018       1.00      0.50      0.67         2\n",
            "        1020       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.49       509\n",
            "   macro avg       0.26      0.29      0.26       509\n",
            "weighted avg       0.47      0.49      0.46       509\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining and Evaluating for Text Tasks:\")\n",
        "text_f1_scores = train_and_evaluate_nn(text_splits, subtask1 + subtask2, model_type='text')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrames for F1 scores for title and text\n",
        "f1_scores_title_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': title_f1_scores\n",
        "})\n",
        "\n",
        "f1_scores_text_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': text_f1_scores\n",
        "})"
      ],
      "metadata": {
        "id": "KKCtxbm4T319"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "id": "wwYyYDcAjBWh",
        "outputId": "7665b767-abe7-4553-f1ea-61602b3e3d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected F1-Scores for Title-Focused Classification:\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.827930\n",
            "1  product-category  0.726340\n",
            "2            hazard  0.635926\n",
            "3           product  0.467829\n",
            "\n",
            "Collected F1-Scores for Text-Focused Classification:\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.941719\n",
            "1  product-category  0.722189\n",
            "2            hazard  0.850044\n",
            "3           product  0.464763\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create DataFrames for F1 scores for title and text\n",
        "f1_scores_title_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': title_f1_scores\n",
        "})\n",
        "\n",
        "f1_scores_text_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': text_f1_scores\n",
        "})\n",
        "\n",
        "# Print the collected F1-scores for title\n",
        "print(\"\\nCollected F1-Scores for Title-Focused Classification:\")\n",
        "print(f1_scores_title_df)\n",
        "\n",
        "# Print the collected F1-scores for text\n",
        "print(\"\\nCollected F1-Scores for Text-Focused Classification:\")\n",
        "print(f1_scores_text_df)\n",
        "\n",
        "# Plot F1-scores for visual comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(f1_scores_title_df['Task'], f1_scores_title_df['F1-Score'], alpha=0.7, label='Title-Focused')\n",
        "plt.bar(f1_scores_text_df['Task'], f1_scores_text_df['F1-Score'], alpha=0.7, label='Text-Focused')\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.title(\"F1-Score Comparison Between Title and Text Tasks\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPzl/4wdhDTw1kMMOywlcGD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}