{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryk42/SemEval-Food-Hazard-Detection-Challenge-2025/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Configuration"
      ],
      "metadata": {
        "id": "0eZi3gaLRgEj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tnUGj2aOD40"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uk4Hr3o6UHO",
        "outputId": "e4c3ba2c-c1ea-4966-a724-a79ca927ef6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting on google drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRqpDhY5OF0r"
      },
      "outputs": [],
      "source": [
        "# Configuration of the model\n",
        "config = {\n",
        "    'max_len': 256,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 0.00005,\n",
        "    'epochs': 100, # Number of training epochs\n",
        "    'model_name': \"dmis-lab/biobert-base-cased-v1.1\"  # BioBERT model\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Z5Prdp5Xrb",
        "outputId": "1e8d22f4-e8d0-4fd7-8bfa-c6504131f89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device for training (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "suuZ5FBS0rhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YaxolIHS5SBH",
        "outputId": "db5d5073-d59c-4910-fedf-882a51f004e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  year  month  day country                             title  \\\n",
            "0           0  1994      1    7      us  Recall Notification: FSIS-024-94   \n",
            "1           1  1994      3   10      us  Recall Notification: FSIS-033-94   \n",
            "2           2  1994      3   28      us  Recall Notification: FSIS-014-94   \n",
            "3           3  1994      4    3      us  Recall Notification: FSIS-009-94   \n",
            "4           4  1994      7    1      us  Recall Notification: FSIS-001-94   \n",
            "\n",
            "                                                text hazard-category  \\\n",
            "0  Case Number: 024-94   \\n            Date Opene...      biological   \n",
            "1  Case Number: 033-94   \\n            Date Opene...      biological   \n",
            "2  Case Number: 014-94   \\n            Date Opene...      biological   \n",
            "3  Case Number: 009-94   \\n            Date Opene...  foreign bodies   \n",
            "4  Case Number: 001-94   \\n            Date Opene...  foreign bodies   \n",
            "\n",
            "               product-category                  hazard  \\\n",
            "0  meat, egg and dairy products  listeria monocytogenes   \n",
            "1  meat, egg and dairy products            listeria spp   \n",
            "2  meat, egg and dairy products  listeria monocytogenes   \n",
            "3  meat, egg and dairy products        plastic fragment   \n",
            "4  meat, egg and dairy products        plastic fragment   \n",
            "\n",
            "                       product  \n",
            "0               smoked sausage  \n",
            "1                      sausage  \n",
            "2                   ham slices  \n",
            "3  thermal processed pork meat  \n",
            "4               chicken breast  \n"
          ]
        }
      ],
      "source": [
        "# URL of the raw file\n",
        "url = \"https://raw.githubusercontent.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/refs/heads/main/data/incidents_train.csv\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6l4Gt-Y6EIH"
      },
      "outputs": [],
      "source": [
        "# Drop the first column\n",
        "df = df.drop(df.columns[0], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsoRrUZg5Xw6",
        "outputId": "544f2a41-2c50-494e-b0ca-c68c53b37393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5082 entries, 0 to 5081\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   year              5082 non-null   int64 \n",
            " 1   month             5082 non-null   int64 \n",
            " 2   day               5082 non-null   int64 \n",
            " 3   country           5082 non-null   object\n",
            " 4   title             5082 non-null   object\n",
            " 5   text              5082 non-null   object\n",
            " 6   hazard-category   5082 non-null   object\n",
            " 7   product-category  5082 non-null   object\n",
            " 8   hazard            5082 non-null   object\n",
            " 9   product           5082 non-null   object\n",
            "dtypes: int64(3), object(7)\n",
            "memory usage: 397.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Check the structure of the DataFrame\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlHfuLKlOQmI"
      },
      "outputs": [],
      "source": [
        "# Function to clean text by removing non-alphanumeric characters and converting to lowercase\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = ' '.join(text.split()) # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to the 'text' column\n",
        "df['text'] = df['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "Pto6qO051Lwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5f2YpcNOVs5"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer for BioBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe4fnbojOY-v"
      },
      "outputs": [],
      "source": [
        "# Define relevant features and targets\n",
        "features = ['year', 'month', 'day', 'country']\n",
        "subtask1 = ['hazard-category','product-category'] # Tasks for subtask 1\n",
        "subtask2 = ['hazard','product'] # Tasks for subtask 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7B8EhUdOer2"
      },
      "outputs": [],
      "source": [
        "# Encode target labels to numeric values\n",
        "label_encoders = {}\n",
        "for target in subtask1 + subtask2:\n",
        "    le = LabelEncoder()\n",
        "    df[target] = le.fit_transform(df[target])  # Fit and transform the target labels\n",
        "    label_encoders[target] = le\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "sgKiz3qL1qwg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbqvZzJgOMmA"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class for PyTorch\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a9dTrQ9Oh7Q"
      },
      "outputs": [],
      "source": [
        "# Function to prepare data splits for training and testing\n",
        "def prepare_data(text_column):\n",
        "    X = df[features + [text_column]] # Combine features with the specified text column\n",
        "    y_subtask1 = df[subtask1]\n",
        "    y_subtask2 = df[subtask2]\n",
        "\n",
        "    data_splits = {}\n",
        "    for target in subtask1 + subtask2:\n",
        "        # Split into training and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, df[target], test_size=0.1, random_state=42\n",
        "        )\n",
        "\n",
        "        # Reset indices for consistency\n",
        "        X_train = X_train.reset_index(drop=True)\n",
        "        y_train = y_train.reset_index(drop=True)\n",
        "        X_test = X_test.reset_index(drop=True)\n",
        "        y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "        data_splits[target] = (X_train, X_test, y_train, y_test)\n",
        "\n",
        "    return data_splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffx_UagwOlb2"
      },
      "outputs": [],
      "source": [
        "# Prepare data splits for title and text tasks\n",
        "title_splits = prepare_data('title')\n",
        "text_splits = prepare_data('text')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "--MiMGs62W1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYXs45FOoPz"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate a model for given tasks\n",
        "def train_and_evaluate_nn(data_splits, targets, model_type='title', early_stopping_patience=6, lr_reduce_factor=0.05):\n",
        "    f1_scores = []  # Store F1-scores for each task\n",
        "\n",
        "    for target in targets:\n",
        "        print(f\"\\nStarting training for task: {target}\")  # Print task message\n",
        "\n",
        "        X_train, X_test, y_train, y_test = data_splits[target]\n",
        "\n",
        "        # Prepare text data using the tokenizer\n",
        "        if model_type == 'title':\n",
        "            texts_train = X_train['title'].values\n",
        "            texts_test = X_test['title'].values\n",
        "        else:\n",
        "            texts_train = X_train['text'].values\n",
        "            texts_test = X_test['text'].values\n",
        "\n",
        "        # Create DataLoader for training and testing\n",
        "        train_dataset = TextDataset(texts_train, y_train, tokenizer, config['max_len'])\n",
        "        test_dataset = TextDataset(texts_test, y_test, tokenizer, config['max_len'])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "        # Model setup\n",
        "        num_labels = len(label_encoders[target].classes_)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=num_labels).to(device)\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=3, verbose=True)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training process\n",
        "        model.train()\n",
        "        best_loss = float('inf')\n",
        "        early_stop_counter = 0\n",
        "\n",
        "        for epoch in range(config['epochs']):\n",
        "            print(f\"Epoch {epoch+1}/{config['epochs']} - Training: {target}\")\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", total=len(train_loader), leave=True)\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = criterion(outputs.logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            # Average loss for the epoch\n",
        "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"Average Training Loss for Epoch {epoch+1}: {avg_epoch_loss}\")\n",
        "\n",
        "            # Evaluate on the test set to compute validation loss\n",
        "            val_loss = 0.0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in test_loader:\n",
        "                    input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                    attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                    labels = batch['label'].to(device)\n",
        "\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(test_loader)\n",
        "            print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss}\")\n",
        "\n",
        "            # Step the scheduler with the validation loss\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_loss:\n",
        "                best_loss = avg_val_loss\n",
        "                early_stop_counter = 0\n",
        "                torch.save(model.state_dict(), f\"best_model_{target}.pt\")  # Save the best model\n",
        "            else:\n",
        "                early_stop_counter += 1\n",
        "                print(f\"Early stopping counter: {early_stop_counter}/{early_stopping_patience}\")\n",
        "\n",
        "            if early_stop_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Load the best model for evaluation\n",
        "        model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n",
        "\n",
        "        # Evaluation process\n",
        "        print(f\"Evaluating model for task: {target}\")\n",
        "        model.eval()\n",
        "        y_preds = []\n",
        "        y_true = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\", total=len(test_loader), leave=True):\n",
        "                input_ids = batch['input_ids'].squeeze(1).to(device)\n",
        "                attention_mask = batch['attention_mask'].squeeze(1).to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                _, preds = torch.max(outputs.logits, dim=1)\n",
        "                y_preds.extend(preds.cpu().numpy())\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Decode labels back to original categories using the label encoder\n",
        "        decoded_preds = label_encoders[target].inverse_transform(y_preds)\n",
        "        decoded_true = label_encoders[target].inverse_transform(y_true)\n",
        "\n",
        "        # Calculate F1 score for the task\n",
        "        f1 = f1_score(decoded_true, decoded_preds, average='weighted')\n",
        "        f1_scores.append(f1)\n",
        "        print(f\"F1-Score for {target}: {f1}\")\n",
        "\n",
        "        # Print classification report\n",
        "        print(f\"Classification Report for {target}:\\n\")\n",
        "        print(classification_report(decoded_true, decoded_preds, zero_division=0))\n",
        "\n",
        "    return f1_scores  # Return the list of F1 scores for plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "95lRv47w3QF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYEB0MQmVAWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b306b6-4bd9-44ba-df91-e2e835029e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Title Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.37it/s, loss=0.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 0.8526037928837162\n",
            "Validation Loss after Epoch 1: 0.6488561267033219\n",
            "Epoch 2/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.49519425415648866\n",
            "Validation Loss after Epoch 2: 0.595657218946144\n",
            "Epoch 3/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.3401822982114981\n",
            "Validation Loss after Epoch 3: 0.6418999996967614\n",
            "Early stopping counter: 1/6\n",
            "Epoch 4/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.537]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.22676551583633348\n",
            "Validation Loss after Epoch 4: 0.6353647615760565\n",
            "Early stopping counter: 2/6\n",
            "Epoch 5/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.273]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.13321629631450282\n",
            "Validation Loss after Epoch 5: 0.7634691828861833\n",
            "Early stopping counter: 3/6\n",
            "Epoch 6/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.00477]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.08040479364417415\n",
            "Validation Loss after Epoch 6: 0.7402717540971935\n",
            "Early stopping counter: 4/6\n",
            "Epoch 7/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.04642826820466459\n",
            "Validation Loss after Epoch 7: 0.7602066892432049\n",
            "Early stopping counter: 5/6\n",
            "Epoch 8/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.00289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.03325558706655629\n",
            "Validation Loss after Epoch 8: 0.7760384087450802\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard-category: 0.7875744067723562\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87       188\n",
            "           1       0.76      0.95      0.85       171\n",
            "           2       0.70      0.60      0.65        35\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       0.85      0.69      0.76        58\n",
            "           5       0.79      0.39      0.52        28\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.88      0.47      0.61        15\n",
            "           9       1.00      0.20      0.33         5\n",
            "\n",
            "    accuracy                           0.80       509\n",
            "   macro avg       0.58      0.42      0.46       509\n",
            "weighted avg       0.80      0.80      0.79       509\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.39it/s, loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 1.8438789517312617\n",
            "Validation Loss after Epoch 1: 1.2205164907500148\n",
            "Epoch 2/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.9666668872733216\n",
            "Validation Loss after Epoch 2: 1.0141451554372907\n",
            "Epoch 3/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.5574097448920871\n",
            "Validation Loss after Epoch 3: 0.9332389263436198\n",
            "Epoch 4/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.3238768228492537\n",
            "Validation Loss after Epoch 4: 1.0378252658993006\n",
            "Early stopping counter: 1/6\n",
            "Epoch 5/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.20429519769716095\n",
            "Validation Loss after Epoch 5: 1.075084287673235\n",
            "Early stopping counter: 2/6\n",
            "Epoch 6/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.1041429436283668\n",
            "Validation Loss after Epoch 6: 1.15093476139009\n",
            "Early stopping counter: 3/6\n",
            "Epoch 7/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0157]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.06087356248310396\n",
            "Validation Loss after Epoch 7: 1.2289086086675525\n",
            "Early stopping counter: 4/6\n",
            "Epoch 8/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0723]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.030466587916081707\n",
            "Validation Loss after Epoch 8: 1.2064156671985984\n",
            "Early stopping counter: 5/6\n",
            "Epoch 9/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.00616]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.023333696284578865\n",
            "Validation Loss after Epoch 9: 1.2085722163319588\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product-category: 0.7238874589526265\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         2\n",
            "           1       0.66      0.74      0.70        54\n",
            "           2       0.72      0.79      0.75        29\n",
            "           3       0.32      0.30      0.31        20\n",
            "           4       0.67      0.55      0.60        11\n",
            "           5       0.50      0.33      0.40         3\n",
            "           6       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.73      0.73      0.73        56\n",
            "          10       0.33      0.89      0.48         9\n",
            "          12       0.97      0.88      0.92        33\n",
            "          13       0.83      0.92      0.88       131\n",
            "          14       1.00      0.62      0.77        16\n",
            "          15       0.93      0.73      0.82        37\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       1.00      0.33      0.50         3\n",
            "          18       0.47      0.37      0.42        51\n",
            "          19       0.89      0.86      0.87        28\n",
            "          20       0.65      0.62      0.63        21\n",
            "          21       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.73       509\n",
            "   macro avg       0.56      0.53      0.52       509\n",
            "weighted avg       0.73      0.73      0.72       509\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.38it/s, loss=2.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.8747779214298808\n",
            "Validation Loss after Epoch 1: 2.1031708363443613\n",
            "Epoch 2/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=2.69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.8562578636032718\n",
            "Validation Loss after Epoch 2: 1.7618765942752361\n",
            "Epoch 3/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.903]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 1.3682830971437734\n",
            "Validation Loss after Epoch 3: 1.6314141619950533\n",
            "Epoch 4/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=1.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 1.0068876291071618\n",
            "Validation Loss after Epoch 4: 1.592162722721696\n",
            "Epoch 5/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=1.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.7172761836017554\n",
            "Validation Loss after Epoch 5: 1.5741683561354876\n",
            "Epoch 6/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.379]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.4978265080705181\n",
            "Validation Loss after Epoch 6: 1.5918682347983122\n",
            "Early stopping counter: 1/6\n",
            "Epoch 7/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.3424900842804488\n",
            "Validation Loss after Epoch 7: 1.6944975974038243\n",
            "Early stopping counter: 2/6\n",
            "Epoch 8/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.2469189565016569\n",
            "Validation Loss after Epoch 8: 1.6649022363126278\n",
            "Early stopping counter: 3/6\n",
            "Epoch 9/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.0454]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.17069614917313405\n",
            "Validation Loss after Epoch 9: 1.7448944002389908\n",
            "Early stopping counter: 4/6\n",
            "Epoch 10/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.081]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.11290546016722032\n",
            "Validation Loss after Epoch 10: 1.7331780698150396\n",
            "Early stopping counter: 5/6\n",
            "Epoch 11/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.0103]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.09654433079311377\n",
            "Validation Loss after Epoch 11: 1.7271518744528294\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard: 0.6353890281175577\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         2\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.33      0.75      0.46         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         2\n",
            "          14       1.00      0.20      0.33         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       0.36      0.76      0.49        17\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          22       1.00      1.00      1.00         3\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          27       1.00      1.00      1.00         1\n",
            "          28       1.00      1.00      1.00         1\n",
            "          33       0.50      0.50      0.50         2\n",
            "          34       0.46      0.81      0.59        16\n",
            "          36       0.68      0.81      0.74        16\n",
            "          38       1.00      0.20      0.33         5\n",
            "          39       1.00      0.50      0.67         2\n",
            "          40       0.75      0.30      0.43        10\n",
            "          42       1.00      1.00      1.00         2\n",
            "          43       0.50      0.25      0.33         4\n",
            "          45       0.50      1.00      0.67         1\n",
            "          47       0.00      0.00      0.00         2\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       1.00      1.00      1.00         4\n",
            "          52       0.70      1.00      0.82         7\n",
            "          54       0.11      0.33      0.17         3\n",
            "          55       0.83      0.85      0.84        75\n",
            "          56       0.00      0.00      0.00         1\n",
            "          57       0.80      0.50      0.62        16\n",
            "          58       0.00      0.00      0.00         2\n",
            "          59       0.86      0.73      0.79        59\n",
            "          61       0.00      0.00      0.00         1\n",
            "          64       1.00      1.00      1.00         1\n",
            "          65       1.00      0.80      0.89         5\n",
            "          68       1.00      1.00      1.00         2\n",
            "          70       0.00      0.00      0.00         2\n",
            "          73       0.38      0.57      0.46        14\n",
            "          75       0.67      0.67      0.67         3\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       1.00      0.33      0.50         3\n",
            "          79       0.00      0.00      0.00         1\n",
            "          80       0.00      0.00      0.00         2\n",
            "          82       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          85       0.59      0.92      0.72        24\n",
            "          86       1.00      0.67      0.80         3\n",
            "          89       0.00      0.00      0.00         2\n",
            "          90       0.57      0.68      0.62        25\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         2\n",
            "          94       0.67      0.50      0.57         4\n",
            "          95       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         3\n",
            "          98       0.76      0.77      0.77        66\n",
            "          99       0.40      0.40      0.40         5\n",
            "         100       0.44      0.47      0.46        17\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.00      0.00      0.00         1\n",
            "         104       1.00      1.00      1.00         1\n",
            "         107       0.00      0.00      0.00         2\n",
            "         108       1.00      0.33      0.50         3\n",
            "         109       0.67      0.91      0.77        11\n",
            "         110       0.00      0.00      0.00         2\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       1.00      1.00      1.00         1\n",
            "         114       0.67      0.80      0.73         5\n",
            "         119       1.00      1.00      1.00         8\n",
            "         121       0.00      0.00      0.00         1\n",
            "         126       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.66       509\n",
            "   macro avg       0.42      0.39      0.38       509\n",
            "weighted avg       0.65      0.66      0.64       509\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:24<00:00,  3.38it/s, loss=6.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 6.245762296489902\n",
            "Validation Loss after Epoch 1: 5.699641019105911\n",
            "Epoch 2/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=5.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 5.096462469000916\n",
            "Validation Loss after Epoch 2: 4.903970472514629\n",
            "Epoch 3/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=4.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 4.17318730671089\n",
            "Validation Loss after Epoch 3: 4.375159651041031\n",
            "Epoch 4/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=3.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 3.428116573737218\n",
            "Validation Loss after Epoch 4: 4.037622183561325\n",
            "Epoch 5/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=3.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 2.7946656546392643\n",
            "Validation Loss after Epoch 5: 3.8166015222668648\n",
            "Epoch 6/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 2.238891651163568\n",
            "Validation Loss after Epoch 6: 3.6735625490546227\n",
            "Epoch 7/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:23<00:00,  3.42it/s, loss=1.24]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 1.7802587830936991\n",
            "Validation Loss after Epoch 7: 3.4671257212758064\n",
            "Epoch 8/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 1.3949574553049529\n",
            "Validation Loss after Epoch 8: 3.399665117263794\n",
            "Epoch 9/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 1.0970371854263585\n",
            "Validation Loss after Epoch 9: 3.315527554601431\n",
            "Epoch 10/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.8699033030575806\n",
            "Validation Loss after Epoch 10: 3.3048871122300625\n",
            "Epoch 11/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.715]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.6901829111409354\n",
            "Validation Loss after Epoch 11: 3.239421673119068\n",
            "Epoch 12/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.5438920856824705\n",
            "Validation Loss after Epoch 12: 3.235342115163803\n",
            "Epoch 13/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.4341181502319299\n",
            "Validation Loss after Epoch 13: 3.25559339299798\n",
            "Early stopping counter: 1/6\n",
            "Epoch 14/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.346710118966607\n",
            "Validation Loss after Epoch 14: 3.2642058059573174\n",
            "Early stopping counter: 2/6\n",
            "Epoch 15/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 15: 0.2807211483718007\n",
            "Validation Loss after Epoch 15: 3.2564377449452877\n",
            "Early stopping counter: 3/6\n",
            "Epoch 16/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 16: 0.22148123844624398\n",
            "Validation Loss after Epoch 16: 3.31188852712512\n",
            "Early stopping counter: 4/6\n",
            "Epoch 17/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 286/286 [01:23<00:00,  3.44it/s, loss=0.102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 17: 0.16965653797468314\n",
            "Validation Loss after Epoch 17: 3.305970475077629\n",
            "Early stopping counter: 5/6\n",
            "Epoch 18/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 286/286 [01:23<00:00,  3.43it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 18: 0.15462044865492133\n",
            "Validation Loss after Epoch 18: 3.302078977227211\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:02<00:00, 11.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product: 0.4739425742763013\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.33      0.50         3\n",
            "           2       0.60      0.60      0.60         5\n",
            "           4       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00         3\n",
            "          19       0.67      1.00      0.80         2\n",
            "          20       0.33      1.00      0.50         1\n",
            "          22       1.00      1.00      1.00         1\n",
            "          23       0.00      0.00      0.00         0\n",
            "          24       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          31       0.00      0.00      0.00         1\n",
            "          35       0.00      0.00      0.00         1\n",
            "          38       0.67      1.00      0.80         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         0\n",
            "          49       0.00      0.00      0.00         1\n",
            "          51       0.50      1.00      0.67         1\n",
            "          55       0.00      0.00      0.00         1\n",
            "          58       0.00      0.00      0.00         3\n",
            "          61       0.40      1.00      0.57         2\n",
            "          62       0.00      0.00      0.00         1\n",
            "          67       0.33      1.00      0.50         1\n",
            "          73       0.00      0.00      0.00         2\n",
            "          74       1.00      1.00      1.00         1\n",
            "          76       0.00      0.00      0.00         0\n",
            "          80       0.00      0.00      0.00         0\n",
            "          84       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         2\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         2\n",
            "          93       0.00      0.00      0.00         2\n",
            "          94       0.00      0.00      0.00         0\n",
            "         108       0.00      0.00      0.00         0\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       0.00      0.00      0.00         0\n",
            "         117       0.78      0.78      0.78         9\n",
            "         119       0.25      0.33      0.29         3\n",
            "         122       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         0\n",
            "         135       0.00      0.00      0.00         1\n",
            "         136       0.00      0.00      0.00         0\n",
            "         138       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00         1\n",
            "         143       0.00      0.00      0.00         1\n",
            "         144       0.00      0.00      0.00         0\n",
            "         149       0.00      0.00      0.00         0\n",
            "         150       0.71      0.71      0.71         7\n",
            "         154       0.00      0.00      0.00         1\n",
            "         157       0.00      0.00      0.00         1\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       0.00      0.00      0.00         3\n",
            "         167       0.58      0.64      0.61        11\n",
            "         168       0.00      0.00      0.00         0\n",
            "         169       0.00      0.00      0.00         1\n",
            "         170       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         1\n",
            "         174       0.00      0.00      0.00         0\n",
            "         177       0.25      0.33      0.29         3\n",
            "         178       0.00      0.00      0.00         0\n",
            "         186       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         4\n",
            "         201       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         0\n",
            "         207       1.00      0.67      0.80         6\n",
            "         209       0.00      0.00      0.00         2\n",
            "         210       1.00      1.00      1.00         1\n",
            "         212       0.60      0.75      0.67         4\n",
            "         219       0.67      1.00      0.80         2\n",
            "         224       1.00      0.67      0.80         3\n",
            "         226       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         0\n",
            "         231       1.00      0.67      0.80         3\n",
            "         239       0.00      0.00      0.00         0\n",
            "         240       1.00      1.00      1.00         1\n",
            "         241       1.00      1.00      1.00         1\n",
            "         245       0.67      1.00      0.80         2\n",
            "         248       1.00      1.00      1.00         1\n",
            "         252       0.00      0.00      0.00         0\n",
            "         255       0.20      1.00      0.33         1\n",
            "         260       0.57      0.80      0.67         5\n",
            "         262       0.00      0.00      0.00         0\n",
            "         263       0.29      0.67      0.40         3\n",
            "         264       0.00      0.00      0.00         1\n",
            "         267       0.00      0.00      0.00         1\n",
            "         268       0.50      1.00      0.67         1\n",
            "         269       0.00      0.00      0.00         0\n",
            "         273       0.00      0.00      0.00         1\n",
            "         278       0.00      0.00      0.00         1\n",
            "         279       1.00      1.00      1.00         1\n",
            "         284       0.00      0.00      0.00         1\n",
            "         286       0.00      0.00      0.00         1\n",
            "         294       0.00      0.00      0.00         0\n",
            "         295       0.75      1.00      0.86         3\n",
            "         296       1.00      1.00      1.00         1\n",
            "         298       1.00      0.50      0.67         2\n",
            "         301       0.33      0.50      0.40         2\n",
            "         302       0.00      0.00      0.00         1\n",
            "         303       0.50      0.50      0.50         2\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       0.75      1.00      0.86         3\n",
            "         313       1.00      1.00      1.00         6\n",
            "         316       1.00      1.00      1.00         1\n",
            "         317       0.00      0.00      0.00         1\n",
            "         319       1.00      1.00      1.00         1\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         346       0.75      0.75      0.75         4\n",
            "         347       0.00      0.00      0.00         1\n",
            "         353       0.00      0.00      0.00         2\n",
            "         354       1.00      1.00      1.00         1\n",
            "         359       0.00      0.00      0.00         0\n",
            "         361       0.00      0.00      0.00         0\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00         1\n",
            "         372       1.00      0.50      0.67         2\n",
            "         373       0.00      0.00      0.00         1\n",
            "         378       0.00      0.00      0.00         1\n",
            "         386       0.50      1.00      0.67         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         1\n",
            "         396       0.50      0.50      0.50         2\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00         1\n",
            "         407       0.00      0.00      0.00         1\n",
            "         415       0.00      0.00      0.00         0\n",
            "         419       0.00      0.00      0.00         1\n",
            "         420       0.00      0.00      0.00         0\n",
            "         428       0.00      0.00      0.00         1\n",
            "         430       1.00      0.50      0.67         2\n",
            "         438       0.00      0.00      0.00         1\n",
            "         439       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         453       0.00      0.00      0.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         2\n",
            "         462       1.00      1.00      1.00         1\n",
            "         463       0.00      0.00      0.00         0\n",
            "         468       1.00      1.00      1.00         1\n",
            "         476       1.00      1.00      1.00         1\n",
            "         485       0.00      0.00      0.00         1\n",
            "         486       0.50      1.00      0.67         1\n",
            "         492       0.50      1.00      0.67         1\n",
            "         494       0.00      0.00      0.00         0\n",
            "         500       0.00      0.00      0.00         1\n",
            "         501       0.00      0.00      0.00         1\n",
            "         502       0.31      0.67      0.42         6\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         508       1.00      1.00      1.00         1\n",
            "         509       0.00      0.00      0.00         1\n",
            "         510       1.00      0.50      0.67         2\n",
            "         520       1.00      0.33      0.50         3\n",
            "         522       0.00      0.00      0.00         0\n",
            "         524       1.00      1.00      1.00         1\n",
            "         530       1.00      0.87      0.93        31\n",
            "         535       1.00      1.00      1.00         2\n",
            "         536       0.00      0.00      0.00         1\n",
            "         537       0.00      0.00      0.00         1\n",
            "         540       1.00      0.67      0.80         3\n",
            "         541       1.00      1.00      1.00         3\n",
            "         543       0.00      0.00      0.00         0\n",
            "         544       0.00      0.00      0.00         0\n",
            "         545       1.00      1.00      1.00         1\n",
            "         547       0.00      0.00      0.00         0\n",
            "         548       0.50      1.00      0.67         1\n",
            "         550       1.00      1.00      1.00         1\n",
            "         554       1.00      1.00      1.00         1\n",
            "         557       1.00      1.00      1.00         1\n",
            "         558       0.00      0.00      0.00         1\n",
            "         566       0.00      0.00      0.00         1\n",
            "         568       1.00      1.00      1.00         1\n",
            "         569       1.00      0.50      0.67         2\n",
            "         571       0.00      0.00      0.00         1\n",
            "         573       0.00      0.00      0.00         1\n",
            "         575       0.00      0.00      0.00         1\n",
            "         576       0.33      1.00      0.50         1\n",
            "         577       0.00      0.00      0.00         0\n",
            "         578       0.00      0.00      0.00         1\n",
            "         580       0.00      0.00      0.00         1\n",
            "         582       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         1\n",
            "         588       0.00      0.00      0.00         1\n",
            "         591       0.29      1.00      0.44         2\n",
            "         593       0.67      1.00      0.80         2\n",
            "         594       0.00      0.00      0.00         1\n",
            "         595       0.00      0.00      0.00         0\n",
            "         597       0.00      0.00      0.00         1\n",
            "         601       1.00      1.00      1.00         1\n",
            "         608       1.00      1.00      1.00         3\n",
            "         609       0.00      0.00      0.00         1\n",
            "         613       0.00      0.00      0.00         1\n",
            "         614       0.00      0.00      0.00         1\n",
            "         615       0.00      0.00      0.00         0\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       0.75      1.00      0.86         3\n",
            "         618       0.00      0.00      0.00         0\n",
            "         621       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         0\n",
            "         623       1.00      1.00      1.00         1\n",
            "         627       0.00      0.00      0.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         631       0.00      0.00      0.00         1\n",
            "         637       0.00      0.00      0.00         0\n",
            "         638       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         647       1.00      1.00      1.00         1\n",
            "         648       1.00      1.00      1.00         2\n",
            "         649       0.60      1.00      0.75         3\n",
            "         650       0.00      0.00      0.00         0\n",
            "         652       0.00      0.00      0.00         1\n",
            "         653       0.00      0.00      0.00         1\n",
            "         654       0.00      0.00      0.00         1\n",
            "         662       0.00      0.00      0.00         2\n",
            "         665       0.38      1.00      0.55         3\n",
            "         666       1.00      0.50      0.67         2\n",
            "         667       1.00      1.00      1.00         1\n",
            "         669       1.00      1.00      1.00         3\n",
            "         670       1.00      1.00      1.00         1\n",
            "         672       1.00      0.50      0.67         2\n",
            "         677       0.00      0.00      0.00         0\n",
            "         679       0.00      0.00      0.00         0\n",
            "         685       0.00      0.00      0.00         1\n",
            "         686       0.40      1.00      0.57         2\n",
            "         690       0.78      1.00      0.88         7\n",
            "         691       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         700       0.75      1.00      0.86         3\n",
            "         706       1.00      1.00      1.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.00      0.00      0.00         1\n",
            "         710       0.00      0.00      0.00         1\n",
            "         711       1.00      1.00      1.00         5\n",
            "         712       0.50      1.00      0.67         1\n",
            "         713       0.00      0.00      0.00         1\n",
            "         714       0.00      0.00      0.00         1\n",
            "         726       0.00      0.00      0.00         1\n",
            "         727       0.00      0.00      0.00         0\n",
            "         728       0.50      1.00      0.67         2\n",
            "         731       0.33      0.50      0.40         2\n",
            "         733       0.00      0.00      0.00         0\n",
            "         734       0.50      0.50      0.50         2\n",
            "         742       0.50      0.50      0.50         2\n",
            "         747       0.00      0.00      0.00         0\n",
            "         748       0.00      0.00      0.00         0\n",
            "         749       0.00      0.00      0.00         3\n",
            "         755       0.00      0.00      0.00         1\n",
            "         756       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         0\n",
            "         762       0.00      0.00      0.00         1\n",
            "         764       1.00      1.00      1.00         1\n",
            "         765       0.00      0.00      0.00         0\n",
            "         770       1.00      1.00      1.00         2\n",
            "         772       0.00      0.00      0.00         1\n",
            "         774       0.00      0.00      0.00         1\n",
            "         775       1.00      1.00      1.00         1\n",
            "         776       0.00      0.00      0.00         3\n",
            "         778       0.00      0.00      0.00         1\n",
            "         781       0.00      0.00      0.00         1\n",
            "         782       0.14      0.20      0.17        10\n",
            "         786       0.00      0.00      0.00         1\n",
            "         788       0.00      0.00      0.00         1\n",
            "         789       0.00      0.00      0.00         1\n",
            "         793       0.00      0.00      0.00         0\n",
            "         796       0.00      0.00      0.00         1\n",
            "         798       0.00      0.00      0.00         0\n",
            "         809       0.00      0.00      0.00         1\n",
            "         810       0.57      0.67      0.62         6\n",
            "         811       1.00      0.50      0.67         2\n",
            "         814       1.00      0.75      0.86         4\n",
            "         815       0.00      0.00      0.00         1\n",
            "         820       0.83      0.83      0.83         6\n",
            "         822       0.00      0.00      0.00         1\n",
            "         825       0.33      0.20      0.25         5\n",
            "         829       0.00      0.00      0.00         1\n",
            "         831       0.00      0.00      0.00         0\n",
            "         832       0.00      0.00      0.00         1\n",
            "         833       0.50      1.00      0.67         1\n",
            "         835       0.00      0.00      0.00         1\n",
            "         837       0.00      0.00      0.00         1\n",
            "         842       0.00      0.00      0.00         0\n",
            "         847       0.00      0.00      0.00         0\n",
            "         848       0.00      0.00      0.00         1\n",
            "         849       0.33      1.00      0.50         1\n",
            "         854       0.67      1.00      0.80         2\n",
            "         858       0.50      1.00      0.67         1\n",
            "         860       1.00      1.00      1.00         1\n",
            "         861       0.50      0.25      0.33         4\n",
            "         863       0.00      0.00      0.00         2\n",
            "         865       0.00      0.00      0.00         0\n",
            "         867       0.00      0.00      0.00         0\n",
            "         870       0.50      1.00      0.67         3\n",
            "         873       0.00      0.00      0.00         1\n",
            "         875       0.00      0.00      0.00         0\n",
            "         885       0.00      0.00      0.00         0\n",
            "         887       0.00      0.00      0.00         1\n",
            "         895       0.50      1.00      0.67         1\n",
            "         896       0.00      0.00      0.00         0\n",
            "         898       0.67      1.00      0.80         2\n",
            "         899       0.00      0.00      0.00         1\n",
            "         903       0.00      0.00      0.00         0\n",
            "         908       1.00      1.00      1.00         1\n",
            "         912       0.67      0.80      0.73         5\n",
            "         916       0.00      0.00      0.00         1\n",
            "         921       0.00      0.00      0.00         1\n",
            "         922       1.00      0.60      0.75         5\n",
            "         923       0.00      0.00      0.00         1\n",
            "         928       0.00      0.00      0.00         0\n",
            "         929       0.00      0.00      0.00         1\n",
            "         930       0.00      0.00      0.00         1\n",
            "         932       0.50      0.50      0.50         2\n",
            "         933       0.40      1.00      0.57         2\n",
            "         939       0.00      0.00      0.00         0\n",
            "         942       0.00      0.00      0.00         0\n",
            "         946       1.00      0.67      0.80         3\n",
            "         950       0.00      0.00      0.00         1\n",
            "         951       0.00      0.00      0.00         2\n",
            "         955       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         966       0.00      0.00      0.00         0\n",
            "         971       0.00      0.00      0.00         1\n",
            "         973       0.00      0.00      0.00         0\n",
            "         978       0.00      0.00      0.00         0\n",
            "         982       0.00      0.00      0.00         1\n",
            "         992       0.00      0.00      0.00         1\n",
            "         998       1.00      1.00      1.00         2\n",
            "         999       1.00      1.00      1.00         1\n",
            "        1000       0.00      0.00      0.00         0\n",
            "        1007       0.50      1.00      0.67         1\n",
            "        1013       0.00      0.00      0.00         1\n",
            "        1017       1.00      1.00      1.00         1\n",
            "        1018       0.00      0.00      0.00         2\n",
            "        1020       0.67      0.67      0.67         3\n",
            "\n",
            "    accuracy                           0.50       509\n",
            "   macro avg       0.27      0.30      0.27       509\n",
            "weighted avg       0.48      0.50      0.47       509\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate for title\n",
        "print(\"\\nTraining and Evaluating for Title Tasks:\")\n",
        "title_f1_scores = train_and_evaluate_nn(title_splits, subtask1 + subtask2, model_type='title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsSoAzzgOyct",
        "outputId": "0e7076d7-860c-444e-9f67-0dd81ab7017e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Evaluating for Text Tasks:\n",
            "\n",
            "Starting training for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.98it/s, loss=0.232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 0.7706871191104809\n",
            "Validation Loss after Epoch 1: 0.46837801905348897\n",
            "Epoch 2/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.265]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 0.31442130603208823\n",
            "Validation Loss after Epoch 2: 0.2639065669500269\n",
            "Epoch 3/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0702]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.1959659682840786\n",
            "Validation Loss after Epoch 3: 0.24787315947469324\n",
            "Epoch 4/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0851]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.14735181523825636\n",
            "Validation Loss after Epoch 4: 0.2584001724026166\n",
            "Early stopping counter: 1/6\n",
            "Epoch 5/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0737]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.1205618751065975\n",
            "Validation Loss after Epoch 5: 0.21943968057166785\n",
            "Epoch 6/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.08742925144123057\n",
            "Validation Loss after Epoch 6: 0.23688465352461208\n",
            "Early stopping counter: 1/6\n",
            "Epoch 7/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.09445655694837077\n",
            "Validation Loss after Epoch 7: 0.2831707904115319\n",
            "Early stopping counter: 2/6\n",
            "Epoch 8/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0038]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.07054947504376459\n",
            "Validation Loss after Epoch 8: 0.2759665162448073\n",
            "Early stopping counter: 3/6\n",
            "Epoch 9/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0249]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.07767248125763128\n",
            "Validation Loss after Epoch 9: 0.243028767312353\n",
            "Early stopping counter: 4/6\n",
            "Epoch 10/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.00573]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.048096020889983\n",
            "Validation Loss after Epoch 10: 0.2189566225570161\n",
            "Epoch 11/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.00579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.02857769754637111\n",
            "Validation Loss after Epoch 11: 0.21899004193983274\n",
            "Early stopping counter: 1/6\n",
            "Epoch 12/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.022779983033220954\n",
            "Validation Loss after Epoch 12: 0.21912154300662223\n",
            "Early stopping counter: 2/6\n",
            "Epoch 13/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.00262]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.019034036523108925\n",
            "Validation Loss after Epoch 13: 0.22276905825856375\n",
            "Early stopping counter: 3/6\n",
            "Epoch 14/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.01515900996315026\n",
            "Validation Loss after Epoch 14: 0.22952274078852497\n",
            "Early stopping counter: 4/6\n",
            "Epoch 15/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.00792]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 15: 0.013088132144062669\n",
            "Validation Loss after Epoch 15: 0.2294168716398417\n",
            "Early stopping counter: 5/6\n",
            "Epoch 16/100 - Training: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.00204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 16: 0.01270870596717496\n",
            "Validation Loss after Epoch 16: 0.22961450154252816\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard-category: 0.9516665880294135\n",
            "Classification Report for hazard-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98       188\n",
            "           1       1.00      0.99      0.99       171\n",
            "           2       0.85      1.00      0.92        35\n",
            "           3       0.00      0.00      0.00         5\n",
            "           4       1.00      0.98      0.99        58\n",
            "           5       0.85      0.82      0.84        28\n",
            "           6       1.00      1.00      1.00         1\n",
            "           7       0.60      1.00      0.75         3\n",
            "           8       0.83      0.67      0.74        15\n",
            "           9       0.75      0.60      0.67         5\n",
            "\n",
            "    accuracy                           0.96       509\n",
            "   macro avg       0.79      0.80      0.79       509\n",
            "weighted avg       0.95      0.96      0.95       509\n",
            "\n",
            "\n",
            "Starting training for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.97it/s, loss=2.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.1979285536946116\n",
            "Validation Loss after Epoch 1: 2.018518630415201\n",
            "Epoch 2/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.5341497301221727\n",
            "Validation Loss after Epoch 2: 1.2957117278128862\n",
            "Epoch 3/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.9902159078763082\n",
            "Validation Loss after Epoch 3: 1.1544393710792065\n",
            "Epoch 4/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.6319804662070074\n",
            "Validation Loss after Epoch 4: 1.006431708112359\n",
            "Epoch 5/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.506]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.4127053035279879\n",
            "Validation Loss after Epoch 5: 1.1475233854725957\n",
            "Early stopping counter: 1/6\n",
            "Epoch 6/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0858]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.27381727714221793\n",
            "Validation Loss after Epoch 6: 1.0357054118067026\n",
            "Early stopping counter: 2/6\n",
            "Epoch 7/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.0254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.18665586605381507\n",
            "Validation Loss after Epoch 7: 1.0345043493434787\n",
            "Early stopping counter: 3/6\n",
            "Epoch 8/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.0186]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.152876228041999\n",
            "Validation Loss after Epoch 8: 1.0867385184392333\n",
            "Early stopping counter: 4/6\n",
            "Epoch 9/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.07538083638227382\n",
            "Validation Loss after Epoch 9: 1.0935583664104342\n",
            "Early stopping counter: 5/6\n",
            "Epoch 10/100 - Training: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.05190129518847574\n",
            "Validation Loss after Epoch 10: 1.1145181995816529\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product-category\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product-category: 0.7371129863519751\n",
            "Classification Report for product-category:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      1.00      0.57         2\n",
            "           1       0.74      0.65      0.69        54\n",
            "           2       0.56      0.79      0.66        29\n",
            "           3       0.37      0.35      0.36        20\n",
            "           4       0.67      0.73      0.70        11\n",
            "           5       1.00      0.33      0.50         3\n",
            "           6       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         1\n",
            "           9       0.83      0.71      0.77        56\n",
            "          10       0.54      0.78      0.64         9\n",
            "          11       0.00      0.00      0.00         0\n",
            "          12       0.83      0.91      0.87        33\n",
            "          13       0.84      0.91      0.88       131\n",
            "          14       1.00      0.75      0.86        16\n",
            "          15       0.92      0.65      0.76        37\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.75      1.00      0.86         3\n",
            "          18       0.55      0.43      0.48        51\n",
            "          19       0.96      0.93      0.95        28\n",
            "          20       0.51      0.86      0.64        21\n",
            "          21       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.74       509\n",
            "   macro avg       0.55      0.56      0.53       509\n",
            "weighted avg       0.75      0.74      0.74       509\n",
            "\n",
            "\n",
            "Starting training for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.97it/s, loss=1.65]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 2.332836267935646\n",
            "Validation Loss after Epoch 1: 1.2381371101364493\n",
            "Epoch 2/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 1.1112476558118434\n",
            "Validation Loss after Epoch 2: 0.9128205412998796\n",
            "Epoch 3/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 0.7712628314909818\n",
            "Validation Loss after Epoch 3: 0.7799744604853913\n",
            "Epoch 4/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.937]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 0.5717579772552619\n",
            "Validation Loss after Epoch 4: 0.7240316862007603\n",
            "Epoch 5/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.545]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 0.4190693744360567\n",
            "Validation Loss after Epoch 5: 0.6807429093169048\n",
            "Epoch 6/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.277]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 0.3433654022539829\n",
            "Validation Loss after Epoch 6: 0.7161517232307233\n",
            "Early stopping counter: 1/6\n",
            "Epoch 7/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.096]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 0.2754847154841869\n",
            "Validation Loss after Epoch 7: 0.6471359225106426\n",
            "Epoch 8/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.568]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 0.22483270096924754\n",
            "Validation Loss after Epoch 8: 0.6630142452777363\n",
            "Early stopping counter: 1/6\n",
            "Epoch 9/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 0.15508749149739742\n",
            "Validation Loss after Epoch 9: 0.6639894533727784\n",
            "Early stopping counter: 2/6\n",
            "Epoch 10/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.13995407334483273\n",
            "Validation Loss after Epoch 10: 0.7371051034424454\n",
            "Early stopping counter: 3/6\n",
            "Epoch 11/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.10832784418016672\n",
            "Validation Loss after Epoch 11: 0.7422684984921943\n",
            "Early stopping counter: 4/6\n",
            "Epoch 12/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0371]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.055243706444630045\n",
            "Validation Loss after Epoch 12: 0.731630021968158\n",
            "Early stopping counter: 5/6\n",
            "Epoch 13/100 - Training: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.0103]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.04423666679482725\n",
            "Validation Loss after Epoch 13: 0.7327738243329804\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: hazard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for hazard: 0.8547971420586762\n",
            "Classification Report for hazard:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.67      1.00      0.80         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       1.00      1.00      1.00         1\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         2\n",
            "          14       1.00      0.80      0.89         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          17       0.80      0.94      0.86        17\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.50      1.00      0.67         1\n",
            "          22       1.00      1.00      1.00         3\n",
            "          23       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          27       0.50      1.00      0.67         1\n",
            "          28       1.00      1.00      1.00         1\n",
            "          33       1.00      0.50      0.67         2\n",
            "          34       0.75      0.94      0.83        16\n",
            "          36       1.00      1.00      1.00        16\n",
            "          38       1.00      0.60      0.75         5\n",
            "          39       0.33      0.50      0.40         2\n",
            "          40       1.00      0.90      0.95        10\n",
            "          42       1.00      1.00      1.00         2\n",
            "          43       0.80      1.00      0.89         4\n",
            "          45       0.50      1.00      0.67         1\n",
            "          47       0.00      0.00      0.00         2\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.50      1.00      0.67         1\n",
            "          50       1.00      1.00      1.00         4\n",
            "          52       0.70      1.00      0.82         7\n",
            "          53       0.00      0.00      0.00         0\n",
            "          54       1.00      1.00      1.00         3\n",
            "          55       0.94      1.00      0.97        75\n",
            "          56       0.00      0.00      0.00         1\n",
            "          57       0.94      0.94      0.94        16\n",
            "          58       1.00      0.50      0.67         2\n",
            "          59       0.97      0.98      0.97        59\n",
            "          61       0.00      0.00      0.00         1\n",
            "          64       1.00      1.00      1.00         1\n",
            "          65       1.00      0.80      0.89         5\n",
            "          67       0.00      0.00      0.00         0\n",
            "          68       1.00      1.00      1.00         2\n",
            "          70       0.33      1.00      0.50         2\n",
            "          73       0.50      0.57      0.53        14\n",
            "          75       0.50      0.33      0.40         3\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       1.00      0.67      0.80         3\n",
            "          79       0.00      0.00      0.00         1\n",
            "          80       0.33      0.50      0.40         2\n",
            "          82       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          85       0.96      0.96      0.96        24\n",
            "          86       1.00      1.00      1.00         3\n",
            "          89       1.00      1.00      1.00         2\n",
            "          90       0.89      0.96      0.92        25\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       1.00      0.50      0.67         2\n",
            "          94       0.80      1.00      0.89         4\n",
            "          95       0.00      0.00      0.00         1\n",
            "          97       0.67      0.67      0.67         3\n",
            "          98       1.00      0.92      0.96        66\n",
            "          99       1.00      1.00      1.00         5\n",
            "         100       0.94      0.88      0.91        17\n",
            "         101       1.00      1.00      1.00         1\n",
            "         102       0.25      1.00      0.40         1\n",
            "         104       1.00      1.00      1.00         1\n",
            "         105       0.00      0.00      0.00         0\n",
            "         107       0.00      0.00      0.00         2\n",
            "         108       1.00      0.67      0.80         3\n",
            "         109       0.73      1.00      0.85        11\n",
            "         110       0.00      0.00      0.00         2\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       1.00      1.00      1.00         1\n",
            "         114       1.00      1.00      1.00         5\n",
            "         116       0.00      0.00      0.00         0\n",
            "         119       1.00      1.00      1.00         8\n",
            "         120       0.00      0.00      0.00         0\n",
            "         121       0.00      0.00      0.00         1\n",
            "         126       1.00      0.67      0.80         3\n",
            "         127       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87       509\n",
            "   macro avg       0.55      0.57      0.54       509\n",
            "weighted avg       0.85      0.87      0.85       509\n",
            "\n",
            "\n",
            "Starting training for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 286/286 [01:36<00:00,  2.97it/s, loss=5.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 1: 6.377010555534096\n",
            "Validation Loss after Epoch 1: 6.035141199827194\n",
            "Epoch 2/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=5.91]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 2: 5.520307906857737\n",
            "Validation Loss after Epoch 2: 5.210012182593346\n",
            "Epoch 3/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=4.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 3: 4.592061935604869\n",
            "Validation Loss after Epoch 3: 4.682246997952461\n",
            "Epoch 4/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=3.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 4: 3.786448148580698\n",
            "Validation Loss after Epoch 4: 4.224446497857571\n",
            "Epoch 5/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=4.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 5: 3.0946482269914\n",
            "Validation Loss after Epoch 5: 3.966480977833271\n",
            "Epoch 6/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=2.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 6: 2.4994481314312327\n",
            "Validation Loss after Epoch 6: 3.747221790254116\n",
            "Epoch 7/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=2.75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 7: 1.9889714859582328\n",
            "Validation Loss after Epoch 7: 3.6067607030272484\n",
            "Epoch 8/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=1.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 8: 1.5600685006255037\n",
            "Validation Loss after Epoch 8: 3.4795343577861786\n",
            "Epoch 9/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.536]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 9: 1.2076855536315825\n",
            "Validation Loss after Epoch 9: 3.4478002935647964\n",
            "Epoch 10/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=0.352]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 10: 0.9421147776025158\n",
            "Validation Loss after Epoch 10: 3.4052760675549507\n",
            "Epoch 11/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 11: 0.7451717789460729\n",
            "Validation Loss after Epoch 11: 3.413219626992941\n",
            "Early stopping counter: 1/6\n",
            "Epoch 12/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 12: 0.5887999204801513\n",
            "Validation Loss after Epoch 12: 3.386452566832304\n",
            "Epoch 13/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 286/286 [01:35<00:00,  3.01it/s, loss=1.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 13: 0.4696628710815123\n",
            "Validation Loss after Epoch 13: 3.348392743617296\n",
            "Epoch 14/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.511]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 14: 0.3708047209294526\n",
            "Validation Loss after Epoch 14: 3.3508796617388725\n",
            "Early stopping counter: 1/6\n",
            "Epoch 15/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 286/286 [01:34<00:00,  3.01it/s, loss=0.361]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 15: 0.29130623029818487\n",
            "Validation Loss after Epoch 15: 3.3615277968347073\n",
            "Early stopping counter: 2/6\n",
            "Epoch 16/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.422]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 16: 0.22497190985750484\n",
            "Validation Loss after Epoch 16: 3.3848554864525795\n",
            "Early stopping counter: 3/6\n",
            "Epoch 17/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 17: 0.17612626324114683\n",
            "Validation Loss after Epoch 17: 3.3919792138040066\n",
            "Early stopping counter: 4/6\n",
            "Epoch 18/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 18: 0.13290434163324275\n",
            "Validation Loss after Epoch 18: 3.3938723541796207\n",
            "Early stopping counter: 5/6\n",
            "Epoch 19/100 - Training: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19: 100%|██████████| 286/286 [01:35<00:00,  3.00it/s, loss=0.0632]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss for Epoch 19: 0.12276766026210222\n",
            "Validation Loss after Epoch 19: 3.3954398222267628\n",
            "Early stopping counter: 6/6\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-cb47ea6c2053>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f\"best_model_{target}.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for task: product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 32/32 [00:03<00:00,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score for product: 0.4632304030796795\n",
            "Classification Report for product:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         3\n",
            "           2       0.50      0.40      0.44         5\n",
            "           4       1.00      1.00      1.00         1\n",
            "           6       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         1\n",
            "          13       1.00      0.67      0.80         3\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       0.25      1.00      0.40         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         0\n",
            "          31       0.00      0.00      0.00         1\n",
            "          35       0.00      0.00      0.00         1\n",
            "          38       0.67      1.00      0.80         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.00      0.00      0.00         0\n",
            "          49       0.50      1.00      0.67         1\n",
            "          51       1.00      1.00      1.00         1\n",
            "          53       0.00      0.00      0.00         0\n",
            "          55       0.00      0.00      0.00         1\n",
            "          58       0.50      0.33      0.40         3\n",
            "          61       0.25      0.50      0.33         2\n",
            "          62       0.00      0.00      0.00         1\n",
            "          66       0.00      0.00      0.00         0\n",
            "          67       1.00      1.00      1.00         1\n",
            "          73       0.00      0.00      0.00         2\n",
            "          74       1.00      1.00      1.00         1\n",
            "          76       0.00      0.00      0.00         0\n",
            "          80       0.00      0.00      0.00         0\n",
            "          84       0.00      0.00      0.00         1\n",
            "          87       0.00      0.00      0.00         2\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       0.00      0.00      0.00         2\n",
            "          93       0.00      0.00      0.00         2\n",
            "          96       0.00      0.00      0.00         0\n",
            "         102       0.00      0.00      0.00         0\n",
            "         108       0.00      0.00      0.00         0\n",
            "         111       0.00      0.00      0.00         1\n",
            "         113       0.00      0.00      0.00         0\n",
            "         114       0.00      0.00      0.00         0\n",
            "         117       0.60      0.67      0.63         9\n",
            "         119       1.00      0.33      0.50         3\n",
            "         122       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         0\n",
            "         135       0.00      0.00      0.00         1\n",
            "         138       0.00      0.00      0.00         1\n",
            "         141       0.00      0.00      0.00         1\n",
            "         143       0.00      0.00      0.00         1\n",
            "         144       0.00      0.00      0.00         0\n",
            "         149       0.00      0.00      0.00         0\n",
            "         150       0.50      0.71      0.59         7\n",
            "         154       0.00      0.00      0.00         1\n",
            "         157       0.00      0.00      0.00         1\n",
            "         159       0.00      0.00      0.00         0\n",
            "         162       0.00      0.00      0.00         0\n",
            "         164       1.00      1.00      1.00         2\n",
            "         165       0.00      0.00      0.00         3\n",
            "         167       0.53      0.73      0.62        11\n",
            "         168       0.00      0.00      0.00         0\n",
            "         169       0.00      0.00      0.00         1\n",
            "         170       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         1\n",
            "         177       0.00      0.00      0.00         3\n",
            "         178       0.00      0.00      0.00         0\n",
            "         186       0.00      0.00      0.00         1\n",
            "         196       0.00      0.00      0.00         4\n",
            "         201       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         0\n",
            "         207       0.33      0.17      0.22         6\n",
            "         209       0.00      0.00      0.00         2\n",
            "         210       1.00      1.00      1.00         1\n",
            "         212       0.43      0.75      0.55         4\n",
            "         219       1.00      0.50      0.67         2\n",
            "         224       1.00      0.67      0.80         3\n",
            "         226       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         0\n",
            "         231       1.00      1.00      1.00         3\n",
            "         240       1.00      1.00      1.00         1\n",
            "         241       0.00      0.00      0.00         1\n",
            "         245       1.00      1.00      1.00         2\n",
            "         248       0.00      0.00      0.00         1\n",
            "         255       0.00      0.00      0.00         1\n",
            "         260       0.80      0.80      0.80         5\n",
            "         262       0.00      0.00      0.00         0\n",
            "         263       0.40      0.67      0.50         3\n",
            "         264       0.00      0.00      0.00         1\n",
            "         267       0.00      0.00      0.00         1\n",
            "         268       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         277       0.00      0.00      0.00         0\n",
            "         278       0.00      0.00      0.00         1\n",
            "         279       0.00      0.00      0.00         1\n",
            "         284       0.00      0.00      0.00         1\n",
            "         286       0.00      0.00      0.00         1\n",
            "         295       0.38      1.00      0.55         3\n",
            "         296       1.00      1.00      1.00         1\n",
            "         298       1.00      0.50      0.67         2\n",
            "         301       0.25      0.50      0.33         2\n",
            "         302       0.00      0.00      0.00         1\n",
            "         303       0.33      0.50      0.40         2\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         309       0.00      0.00      0.00         1\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       0.75      1.00      0.86         3\n",
            "         313       0.75      1.00      0.86         6\n",
            "         316       1.00      1.00      1.00         1\n",
            "         317       0.00      0.00      0.00         1\n",
            "         319       1.00      1.00      1.00         1\n",
            "         324       0.00      0.00      0.00         0\n",
            "         329       0.00      0.00      0.00         0\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.50      1.00      0.67         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         346       0.80      1.00      0.89         4\n",
            "         347       0.00      0.00      0.00         1\n",
            "         350       0.00      0.00      0.00         0\n",
            "         353       0.00      0.00      0.00         2\n",
            "         354       1.00      1.00      1.00         1\n",
            "         357       0.00      0.00      0.00         0\n",
            "         360       0.00      0.00      0.00         0\n",
            "         361       0.00      0.00      0.00         0\n",
            "         367       0.00      0.00      0.00         1\n",
            "         368       0.00      0.00      0.00         1\n",
            "         372       0.50      0.50      0.50         2\n",
            "         373       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         0\n",
            "         378       0.00      0.00      0.00         1\n",
            "         386       0.50      1.00      0.67         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         1\n",
            "         396       0.00      0.00      0.00         2\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         1\n",
            "         406       0.00      0.00      0.00         1\n",
            "         407       0.00      0.00      0.00         1\n",
            "         409       0.00      0.00      0.00         0\n",
            "         419       0.00      0.00      0.00         1\n",
            "         427       0.00      0.00      0.00         0\n",
            "         428       0.00      0.00      0.00         1\n",
            "         430       0.00      0.00      0.00         2\n",
            "         432       0.00      0.00      0.00         0\n",
            "         438       0.00      0.00      0.00         1\n",
            "         439       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         443       0.00      0.00      0.00         0\n",
            "         453       1.00      1.00      1.00         1\n",
            "         455       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         2\n",
            "         460       0.00      0.00      0.00         0\n",
            "         461       0.00      0.00      0.00         0\n",
            "         462       1.00      1.00      1.00         1\n",
            "         466       0.00      0.00      0.00         0\n",
            "         468       1.00      1.00      1.00         1\n",
            "         476       1.00      1.00      1.00         1\n",
            "         485       0.00      0.00      0.00         1\n",
            "         486       1.00      1.00      1.00         1\n",
            "         492       0.50      1.00      0.67         1\n",
            "         494       0.00      0.00      0.00         0\n",
            "         495       0.00      0.00      0.00         0\n",
            "         500       0.00      0.00      0.00         1\n",
            "         501       0.00      0.00      0.00         1\n",
            "         502       0.75      1.00      0.86         6\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         508       0.00      0.00      0.00         1\n",
            "         509       0.00      0.00      0.00         1\n",
            "         510       1.00      1.00      1.00         2\n",
            "         520       1.00      0.67      0.80         3\n",
            "         524       1.00      1.00      1.00         1\n",
            "         529       0.00      0.00      0.00         0\n",
            "         530       0.97      0.90      0.93        31\n",
            "         535       1.00      1.00      1.00         2\n",
            "         536       0.00      0.00      0.00         1\n",
            "         537       0.50      1.00      0.67         1\n",
            "         540       0.75      1.00      0.86         3\n",
            "         541       1.00      1.00      1.00         3\n",
            "         543       0.00      0.00      0.00         0\n",
            "         545       1.00      1.00      1.00         1\n",
            "         547       0.00      0.00      0.00         0\n",
            "         548       0.00      0.00      0.00         1\n",
            "         550       0.00      0.00      0.00         1\n",
            "         554       1.00      1.00      1.00         1\n",
            "         557       1.00      1.00      1.00         1\n",
            "         558       0.00      0.00      0.00         1\n",
            "         559       0.00      0.00      0.00         0\n",
            "         562       0.00      0.00      0.00         0\n",
            "         566       0.00      0.00      0.00         1\n",
            "         568       0.50      1.00      0.67         1\n",
            "         569       1.00      0.50      0.67         2\n",
            "         570       0.00      0.00      0.00         0\n",
            "         571       0.00      0.00      0.00         1\n",
            "         573       0.00      0.00      0.00         1\n",
            "         575       0.00      0.00      0.00         1\n",
            "         576       0.33      1.00      0.50         1\n",
            "         577       0.00      0.00      0.00         0\n",
            "         578       0.00      0.00      0.00         1\n",
            "         580       0.00      0.00      0.00         1\n",
            "         582       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         1\n",
            "         588       0.00      0.00      0.00         1\n",
            "         591       0.29      1.00      0.44         2\n",
            "         593       0.67      1.00      0.80         2\n",
            "         594       0.00      0.00      0.00         1\n",
            "         597       0.00      0.00      0.00         1\n",
            "         601       0.00      0.00      0.00         1\n",
            "         608       0.00      0.00      0.00         3\n",
            "         609       0.00      0.00      0.00         1\n",
            "         613       0.00      0.00      0.00         1\n",
            "         614       0.00      0.00      0.00         1\n",
            "         615       0.00      0.00      0.00         0\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       1.00      0.67      0.80         3\n",
            "         618       0.00      0.00      0.00         0\n",
            "         621       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         0\n",
            "         623       1.00      1.00      1.00         1\n",
            "         627       0.00      0.00      0.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         631       0.00      0.00      0.00         1\n",
            "         638       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         647       1.00      1.00      1.00         1\n",
            "         648       1.00      0.50      0.67         2\n",
            "         649       0.67      0.67      0.67         3\n",
            "         652       0.00      0.00      0.00         1\n",
            "         653       0.00      0.00      0.00         1\n",
            "         654       0.00      0.00      0.00         1\n",
            "         662       0.00      0.00      0.00         2\n",
            "         665       0.40      0.67      0.50         3\n",
            "         666       0.25      0.50      0.33         2\n",
            "         667       0.33      1.00      0.50         1\n",
            "         669       1.00      1.00      1.00         3\n",
            "         670       1.00      1.00      1.00         1\n",
            "         672       1.00      0.50      0.67         2\n",
            "         680       0.00      0.00      0.00         0\n",
            "         685       0.00      0.00      0.00         1\n",
            "         686       0.25      0.50      0.33         2\n",
            "         690       0.83      0.71      0.77         7\n",
            "         691       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         700       0.75      1.00      0.86         3\n",
            "         706       1.00      1.00      1.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.33      1.00      0.50         1\n",
            "         710       0.00      0.00      0.00         1\n",
            "         711       1.00      1.00      1.00         5\n",
            "         712       0.00      0.00      0.00         1\n",
            "         713       0.00      0.00      0.00         1\n",
            "         714       0.00      0.00      0.00         1\n",
            "         726       0.00      0.00      0.00         1\n",
            "         727       0.00      0.00      0.00         0\n",
            "         728       0.67      1.00      0.80         2\n",
            "         731       0.00      0.00      0.00         2\n",
            "         734       0.50      1.00      0.67         2\n",
            "         742       0.50      0.50      0.50         2\n",
            "         743       0.00      0.00      0.00         0\n",
            "         744       0.00      0.00      0.00         0\n",
            "         748       0.00      0.00      0.00         0\n",
            "         749       0.00      0.00      0.00         3\n",
            "         755       1.00      1.00      1.00         1\n",
            "         756       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         0\n",
            "         762       0.00      0.00      0.00         1\n",
            "         764       0.00      0.00      0.00         1\n",
            "         770       1.00      1.00      1.00         2\n",
            "         772       0.00      0.00      0.00         1\n",
            "         774       0.00      0.00      0.00         1\n",
            "         775       1.00      1.00      1.00         1\n",
            "         776       0.00      0.00      0.00         3\n",
            "         778       0.00      0.00      0.00         1\n",
            "         781       0.00      0.00      0.00         1\n",
            "         782       0.25      0.30      0.27        10\n",
            "         786       0.00      0.00      0.00         1\n",
            "         788       0.00      0.00      0.00         1\n",
            "         789       0.00      0.00      0.00         1\n",
            "         793       0.00      0.00      0.00         0\n",
            "         796       0.00      0.00      0.00         1\n",
            "         798       0.00      0.00      0.00         0\n",
            "         800       0.00      0.00      0.00         0\n",
            "         809       0.00      0.00      0.00         1\n",
            "         810       0.50      0.67      0.57         6\n",
            "         811       1.00      1.00      1.00         2\n",
            "         814       0.75      0.75      0.75         4\n",
            "         815       1.00      1.00      1.00         1\n",
            "         816       0.00      0.00      0.00         0\n",
            "         820       1.00      0.83      0.91         6\n",
            "         822       0.00      0.00      0.00         1\n",
            "         825       0.67      0.40      0.50         5\n",
            "         829       0.00      0.00      0.00         1\n",
            "         831       0.00      0.00      0.00         0\n",
            "         832       0.00      0.00      0.00         1\n",
            "         833       0.50      1.00      0.67         1\n",
            "         835       0.00      0.00      0.00         1\n",
            "         837       0.00      0.00      0.00         1\n",
            "         842       0.00      0.00      0.00         0\n",
            "         846       0.00      0.00      0.00         0\n",
            "         848       0.00      0.00      0.00         1\n",
            "         849       0.50      1.00      0.67         1\n",
            "         854       0.67      1.00      0.80         2\n",
            "         857       0.00      0.00      0.00         0\n",
            "         858       0.50      1.00      0.67         1\n",
            "         860       0.50      1.00      0.67         1\n",
            "         861       1.00      0.25      0.40         4\n",
            "         863       0.00      0.00      0.00         2\n",
            "         867       0.00      0.00      0.00         0\n",
            "         870       0.50      1.00      0.67         3\n",
            "         873       0.00      0.00      0.00         1\n",
            "         887       0.00      0.00      0.00         1\n",
            "         895       1.00      1.00      1.00         1\n",
            "         898       1.00      1.00      1.00         2\n",
            "         899       1.00      1.00      1.00         1\n",
            "         903       0.00      0.00      0.00         0\n",
            "         908       0.50      1.00      0.67         1\n",
            "         912       0.80      0.80      0.80         5\n",
            "         916       0.00      0.00      0.00         1\n",
            "         921       0.00      0.00      0.00         1\n",
            "         922       0.43      0.60      0.50         5\n",
            "         923       0.00      0.00      0.00         1\n",
            "         928       0.00      0.00      0.00         0\n",
            "         929       0.00      0.00      0.00         1\n",
            "         930       0.00      0.00      0.00         1\n",
            "         932       0.00      0.00      0.00         2\n",
            "         933       1.00      1.00      1.00         2\n",
            "         942       0.00      0.00      0.00         0\n",
            "         945       0.00      0.00      0.00         0\n",
            "         946       1.00      0.67      0.80         3\n",
            "         950       1.00      1.00      1.00         1\n",
            "         951       0.00      0.00      0.00         2\n",
            "         952       0.00      0.00      0.00         0\n",
            "         955       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         971       0.00      0.00      0.00         1\n",
            "         973       0.00      0.00      0.00         0\n",
            "         982       0.00      0.00      0.00         1\n",
            "         992       0.00      0.00      0.00         1\n",
            "         998       1.00      1.00      1.00         2\n",
            "         999       0.50      1.00      0.67         1\n",
            "        1007       0.50      1.00      0.67         1\n",
            "        1013       1.00      1.00      1.00         1\n",
            "        1017       1.00      1.00      1.00         1\n",
            "        1018       1.00      0.50      0.67         2\n",
            "        1020       1.00      0.67      0.80         3\n",
            "\n",
            "    accuracy                           0.49       509\n",
            "   macro avg       0.26      0.29      0.27       509\n",
            "weighted avg       0.47      0.49      0.46       509\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate for text\n",
        "print(\"\\nTraining and Evaluating for Text Tasks:\")\n",
        "text_f1_scores = train_and_evaluate_nn(text_splits, subtask1 + subtask2, model_type='text')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "wTPZpMRl2_6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwYyYDcAjBWh",
        "outputId": "2bfc4d9f-6cbd-47be-d388-1d8a63415c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected F1-Scores for Title-Focused Classification:\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.787574\n",
            "1  product-category  0.723887\n",
            "2            hazard  0.635389\n",
            "3           product  0.473943\n",
            "\n",
            "Collected F1-Scores for Text-Focused Classification:\n",
            "               Task  F1-Score\n",
            "0   hazard-category  0.951667\n",
            "1  product-category  0.737113\n",
            "2            hazard  0.854797\n",
            "3           product  0.463230\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create DataFrames for F1 scores for title and text\n",
        "f1_scores_title_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': title_f1_scores\n",
        "})\n",
        "\n",
        "f1_scores_text_df = pd.DataFrame({\n",
        "    'Task': subtask1 + subtask2,\n",
        "    'F1-Score': text_f1_scores\n",
        "})\n",
        "\n",
        "# Print the collected F1-scores for title\n",
        "print(\"\\nCollected F1-Scores for Title-Focused Classification:\")\n",
        "print(f1_scores_title_df)\n",
        "\n",
        "# Print the collected F1-scores for text\n",
        "print(\"\\nCollected F1-Scores for Text-Focused Classification:\")\n",
        "print(f1_scores_text_df)\n",
        "\n",
        "# Plot F1-scores for visual comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(f1_scores_title_df['Task'], f1_scores_title_df['F1-Score'], alpha=0.7, label='Title-Focused')\n",
        "plt.bar(f1_scores_text_df['Task'], f1_scores_text_df['F1-Score'], alpha=0.7, label='Text-Focused')\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.title(\"F1-Score Comparison Between Title and Text Tasks\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Text-Focused approach consistently achieves better or comparable F1-scores across all tasks, suggesting that incorporating more text-based features enhances the model's ability to classify or predict effectively in these tasks. Tasks involving \"product\" appear to be more challenging than \"category\" tasks overall, as seen in their lower F1-scores."
      ],
      "metadata": {
        "id": "P1PrsM_zzO-n"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOI6I092ZOHahW8PsA9V5M7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
